<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nasirlukman.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nasirlukman.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-08T10:42:16+00:00</updated><id>https://nasirlukman.github.io/feed.xml</id><title type="html">Earth.etc</title><subtitle></subtitle><entry><title type="html">Satellite Embeddings and Large Scale Model</title><link href="https://nasirlukman.github.io/blog/2025/satellite-embeddings/" rel="alternate" type="text/html" title="Satellite Embeddings and Large Scale Model"/><published>2025-11-07T23:36:10+00:00</published><updated>2025-11-07T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/satellite-embeddings</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/satellite-embeddings/"><![CDATA[<p>A few months ago, the remote sensing world was shaken by one of Google DeepMind‚Äôs most ambitious Earth observation projects: the official release of <a href="https://medium.com/google-earth/ai-powered-pixels-introducing-googles-satellite-embedding-dataset-31744c1f4650">Google Satellite Embeddings</a>. Just hours after the announcement, I dove into the white paper and tested the data myself, and I was genuinely blown away. I was also lucky enough to get a seat in the <a href="https://earthoutreachonair.withgoogle.com/events/geoforgood25-singapore">Geo for Good 2025 Summit</a> in Singapore, where they introduced the dataset and showcased its capabilities in person.</p> <p>To be fair, the concept of embeddings isn‚Äôt new. Even in remote sensing, several research groups have developed their own embeddings from satellite data to better represent Earth‚Äôs surface. But what makes Google‚Äôs version stand out is its sheer scale and global coverage. It‚Äôs the first dataset (to my knowledge) that captures such a broad and consistent representation of the planet at once.</p> <p>There are already plenty of great technical explanations about how satellite embeddings are produced (their <a href="https://arxiv.org/abs/2507.22291">white paper</a> is the main source of good information, of course), so I won‚Äôt repeat them here. To put it simply: I like to think of embeddings as ‚ÄúPCA on steroids.‚Äù They transform data into a new (usually lower-dimensional) space that preserves essential information while making it easier for machines to process. The tradeoff is that each dimension becomes more abstract. It is harder for us mortals to interpret, but far more powerful for computers to use. By the way, why does it sounds like we are <a href="https://bmiddleton1.substack.com/p/the-ouroboros-effect-how-ai-threatens">sacrificing knowledge for power</a>?</p> <p>Anway, recently I‚Äôve been working on a rather ambitious project which involves in building an annual, nationwide land use/land cover (LULC) model for Indonesia. The idea is simple: users can draw a boundary anywhere in Indonesia, and the system will automatically generate a land cover map for that area, historically (from 2017) up to the latest available year. Ready for further analysis.</p> <p>I aim for detailed classification, covering major agricultural commodities such as palm oil, acacia, coffee, and rubber, along with different forest types and other distinctive land covers.</p> <p>Before embeddings were available, I had built several LULC models (like the one I wrote about <a href="https://nasirlukman.github.io/blog/2025/OBIA-LULC/">here</a>) but they were mostly localized due to resource constraints constraints.</p> <p>Now, with satellite embeddings, and of course along with other advances like open labeled datasets, multi-sensor imagery, and Google Earth Engine, building large-scale models has become much more achievable for small organizations or individual researchers like myself. Embeddings remove most of the heavy feature engineering and feature selection work, while also encoding spatial context (neighborhood relationships) and temporal patterns that capture vegetation phenology. These make simpler pixel based machine learning methdod on top of it become so much better and more powerful.</p> <p>Below is an example of my model in action, along with its accuracy metrics:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_14_classification_result-480.webp 480w,/assets/img/post_14_classification_result-800.webp 800w,/assets/img/post_14_classification_result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_14_classification_result.png" class="img-fluid rounded" width="100%" height="auto" alt="Sentinel 2 Annual Cloudless Composite, Google satellite embeddings, and land use land cover classification results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Example of classification result on a small patch of land in Borneo. (Sentinel 2 annual cloudless mosaic on top, Google Satelite embedings false color composite in the middle, and land use/land cover classification result at the bottom). </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_14_accuracy_assesment-480.webp 480w,/assets/img/post_14_accuracy_assesment-800.webp 800w,/assets/img/post_14_accuracy_assesment-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_14_accuracy_assesment.png" class="img-fluid rounded" width="100%" height="auto" alt="Tabel showing the accuracy metrics (preccision, recall, f1 score, and support for each class. the model have 0 )" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Accuracy metric. The Overall Accuracy (OA) is 0.88. Not bad for a large scale model üòâ. </div> <h2 id="looking-ahead">Looking Ahead</h2> <p>What‚Äôs next for Earth observation? I believe this is only the beginning. Soon, we‚Äôll likely see open embeddings with shorter temporal spans (maybe monthly?) and even embeddings incorporating richer data input such as hyperspectral data for example.</p> <p>As a remote sensing scientist, this kind of progress excites me. It feels like we‚Äôre entering a new era where powerful global datasets allow small teams or even individuals to tackle problems that once required huge resources.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="machine_learning,"/><category term="image_analysis"/><summary type="html"><![CDATA[Brief note on my experience in upscaling my model using Google Satelite Embedings]]></summary></entry><entry><title type="html">From Arrays to Graph: Different Ways to Represent Image Data in Remote Sensing</title><link href="https://nasirlukman.github.io/blog/2025/image-data-representation/" rel="alternate" type="text/html" title="From Arrays to Graph: Different Ways to Represent Image Data in Remote Sensing"/><published>2025-07-12T23:36:10+00:00</published><updated>2025-07-12T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/image-data-representation</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/image-data-representation/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>In the geospatial and remote sensing world, image files are typically referred to as raster files. These files store images in pixels, where each pixel is located at a specific row and column position and stores a single value representing intensity or reflectance. This structure resembles a 2D matrix or array. If each pixel is associated with multiple values (like in multispectral or hyperspectral data) they are stored across different bands, which are essentially multiple aligned layers, each representing one variable.</p> <p>These raster files are always accompanied by metadata, which includes the coordinate system, pixel resolution, and the coordinates of a reference point in the image (typically the top-left corner). With this information, we can geolocate each pixel. Raster data is typically stored in .tif files, and when geospatial metadata is included, the file is referred to as a GeoTIFF. This format is widely supported and convenient for storing remote sensing data.</p> <p>More modern formats like <a href="https://cogeo.org/">Cloud Optimized GeoTIFFs (COGs)</a> allow for efficient workflows in cloud-based environments, which is crucial considering the rapidly increasing volume of satellite imagery. Still, at their core, these files follow the same basic structure: an array of pixel values.</p> <p>As users of the data, we have the freedom to represent and structure it in different ways, depending on the needs of our analysis. Alternative representations can open up new ways of thinking about image analysis and allow us to apply analytical methods that may not work well with standard raster formats. In this post, we‚Äôll discuss different ways to think about and structure our image data, and how these representations support different types of analysis.</p> <p>For this example, we‚Äôll use a small image patch with three bands to simplify the visualizations. We downsample this small patch of forest and grassland into a 5√ó5 pixel image, so the total shape is 5√ó5√ó3.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_resample-480.webp 480w,/assets/img/post_13_resample-800.webp 800w,/assets/img/post_13_resample-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_resample.png" class="img-fluid rounded" width="100%" height="auto" alt="Satelite Imagery from Planet that shows a patch of forest and grassland. it is downsampled to show the whole area as 5x5 pixel for simplicity in the data representation example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Example image we will use as an example, for simplicity we will resample this image into 5 by 5 pixels and only use RGB bands resulting in array with shape of 5x5x3. </div> <h1 id="1-data-cube">1. Data Cube</h1> <p>The most natural way to represent multiband image data is probably as an array. Our previous image can be visualized as a 5√ó5√ó1 array like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_composite-480.webp 480w,/assets/img/post_13_composite-800.webp 800w,/assets/img/post_13_composite-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_composite.png" class="img-fluid rounded" width="100%" height="auto" alt="RGb Composite image composite visualize as a 5x5x1 voxels" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. The same image visualize as 5x5x1 voxels where the three RGB bands are reduced into one color composite bands. </div> <p>In this case, the displayed pixel values, which shown as colors, are not the original data but a composite of the RGB bands. Behind the scenes, the raw data is a stack of three 5√ó5 grids, where each layer represents the intensity of red, green, and blue light respectively. Since human eyes perceive light in these wavelengths, we see a combination of them as color. This RGB composite is helpful for visualization, as it compresses information in a way we can easily interpret. However, this compression also leads to some information loss. That‚Äôs why, for image analysis, we usually work with the raw, uncompressed form.</p> <p>In raw form, the image is a 5√ó5√ó3 array. For the computer, this is just a structured sequence of numbers. But it helps us to imagine it as a cube, where the horizontal plane represents the x and y coordinates, and the vertical axis represents the intensity in each spectral band (in this case, red, green, and blue).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_datacube-480.webp 480w,/assets/img/post_13_datacube-800.webp 800w,/assets/img/post_13_datacube-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_datacube.png" class="img-fluid rounded" width="100%" height="auto" alt="5x5x3 voxels representation of raw image array. Also often reffered to as datacube. In this example each bands that represent light intensity of red, gree, and blue light respectively are stacked together" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. The same image visualize as 5x5x3 voxels of the raw data. This often reffered to as data cube. </div> <p>This 3D structure is often referred to as a tensor. Representing data in this way is useful if we want to apply linear algebra or use models that rely on spatial and spectral structure. For example, in deep learning, <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks (CNNs)</a> apply a series of convolution operations (visualized as yellow-lined voxels in the image 4) to extract spatial and spectral features from the image. These filters slide over the image with the goal of capturing shapes, textures, and color combinations that define particular classes or objects.</p> <p>This approach is very powerful and currently forms the state of the art in image analysis. However, it comes with limitations, especially the need for large labeled datasets and substantial computational power.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_convolution_anim-480.webp 480w,/assets/img/post_13_convolution_anim-800.webp 800w,/assets/img/post_13_convolution_anim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_convolution_anim.gif" class="img-fluid rounded" width="100%" height="auto" alt=".gif animation that represent simplified animation of how convolution layer in CNN deep learning works" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Animated example of how convolutional layer moves through the image to capture complex spatial and spectral patter of the image. </div> <h1 id="2-spatially-flattened-matrix">2. Spatially Flattened Matrix</h1> <p>Some machine learning algorithms like <a href="https://scikit-learn.org/stable/modules/svm.html">SVM</a>, <a href="https://scikit-learn.org/stable/modules/ensemble.html#forest">Random Forest</a>, and <a href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html">XGBoost</a> do not take spatial patterns into account. They treat each pixel independently and classify them based only on their spectral signatures.</p> <p>If we‚Äôre using such methods, or any approach that works on a pixel-by-pixel basis, then maintaining the spatial relationship between pixels is unnecessary. Instead, we can restructure the image as a 2D array, where each column represents an individual pixel, and each row represents a spectral band.</p> <p>The computer will interpret this as just another structured collection of numbers, though with a simpler 2D format.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_pixelwise_classification_anim-480.webp 480w,/assets/img/post_13_pixelwise_classification_anim-800.webp 800w,/assets/img/post_13_pixelwise_classification_anim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_pixelwise_classification_anim.gif" class="img-fluid rounded" width="100%" height="auto" alt=".gif animation that represent simplified animation typical pixel-by-pixel classifier such as Random Forest, SVM, XGBoost, moves from one pixel to the other and treat each pixel as individual vector in the feature space." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. Animated example of how typical machine learning classifier moves from one pixel to another in the spatially flattened matrix, treating each pixel as a vector. </div> <p>This representation allows the classifier to treat each column as a feature vector, and move from one pixel to another without worrying about their positions. It‚Äôs efficient and works well when spatial context isn‚Äôt critical.</p> <h1 id="3-collection-of-vectors-in-feature-space">3. Collection of Vectors in Feature Space</h1> <p>In the previous example, we already treated each pixel as a vector, but we still thought of it mainly as numerical data. In this section, we look at how visualizing each pixel as a geometric object can be useful.</p> <p>Using the same image, we now represent each pixel as a vector from origin in RGB space.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_vectors-480.webp 480w,/assets/img/post_13_vectors-800.webp 800w,/assets/img/post_13_vectors-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_vectors.png" class="img-fluid rounded" width="100%" height="auto" alt="image representations as vectors in feature space. This type of representation are useful for geometric analysis of the data which is the basis of Spectral Angle Mapper (SAM), K-means Classification, and Principle Ocmponent Analysis (PCA)" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. In this representation, each pixel are visualized as vectors which start from origin (0,0,0) to the data coordinate in RGB feature space. </div> <p>This allows us to evaluate similarity between pixels based on the angle between vectors, also known as angular distance (i‚Äôve discussed this in more detail before <a href="https://nasirlukman.github.io/blog/2024/distance/">here</a>). Pixels with the same color but different brightness will have the same direction (0¬∞ angular distance), and only differ in magnitude. This is particularly useful in remote sensing, where terrain and illumination changes can cause variations in brightness. Using the <a href="https://step.esa.int/main/wp-content/help/versions/9.0.0/snap-toolboxes/org.esa.s2tbx.s2tbx.spectral.angle.mapper.ui/sam/SAMProcessor.html">Spectral Angle Mapper (SAM)</a> method, these brightness differences can be ignored.</p> <p>This geometric interpretation of pixel vectors is also useful for other analytical methods like <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means clustering</a> and <a href="https://pro.arcgis.com/en/pro-app/3.3/tool-reference/spatial-analyst/how-principal-components-works.htm">Principle Component Analysis (PCA)</a>, both of which are based on geometry analysis of pixels in feature space.</p> <h1 id="4-graphs">4. Graphs</h1> <p>Another way to represent an image is as a graph. A graph consists of nodes and edges. Each node represents a pixel and stores its spectral information. Edges represent connections between pixels, and each edge has a weight that reflects the similarity between the connected pixels.</p> <p>We have flexibility in how we define these graphs. In this example, I use the pixel‚Äôs spatial coordinates for node positions. Each node connects to its 8 neighbors (horizontal, vertical, and diagonal), and the edge weight is calculated based on both spatial distance and spectral similarity (i.e. pixel that is closer to each other and have similar color/spectra have the lowest weight)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_spatial_graph-480.webp 480w,/assets/img/post_13_spatial_graph-800.webp 800w,/assets/img/post_13_spatial_graph-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_spatial_graph.png" class="img-fluid rounded" width="100%" height="auto" alt="image representations as graph in image spatial space. each node are located in the geographical coordinate of the pixel and contain all of its spectral information and each edge are weighted by its spatial closeness and spectral similarity" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. Graph representation of image in spatial space. </div> <p>Even with this simple setup, we can already capture the spatial and spectral structure of the image. For example, superpixel clustering can be done by cutting edges based on weight thresholds. More complex methods like graph-based deep learning also become possible with this structure.</p> <p>In remote sensing, Object-Based Image Analysis (OBIA) is often used for tasks like land cover classification. OBIA captures both spatial and spectral patterns and is widely used because it‚Äôs less demanding than deep learning but still effective. Typically, clustering algorithms like <a href="https://developers.google.com/earth-engine/apidocs/ee-algorithms-image-segmentation-snic">SNIC</a> are used first, followed by supervised classification methods like Random Forest or SVM applied to each cluster.</p> <p>I think OBIA could be more naturally implemented using a graph-based approach, where clustering and classification happen simultaneously in a unified framework. However, I haven‚Äôt explored this idea in depth myself.</p> <p>We can also define the graph purely in feature space, where nodes are pixels and edge weights are the Euclidean distance between spectral vectors. This structure makes it easy to apply unsupervised classification methods that group pixels based on spectral similarity, much like K-means.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_13_spectral_graph-480.webp 480w,/assets/img/post_13_spectral_graph-800.webp 800w,/assets/img/post_13_spectral_graph-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_13_spectral_graph.png" class="img-fluid rounded" width="100%" height="auto" alt="image representation as graph in feature space. each node are located in the feature space according to the feature values. Each edge sinmply weighted by euclidean distance." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8. Graph representation of image in feature space. </div> <h1 id="takeaways">Takeaways</h1> <p>The way we represent image data can influence how we think about and analyze it. Whether we treat the image as a cube, a flat table, a set of vectors, or a graph, each perspective opens different possibilities and supports different types of methods. In practice, we often move between these representations depending on the task. Some problems require spatial context, some don‚Äôt, and some benefit from looking at geometry in feature space.</p> <p>In this post, I simplified the image to just a few pixels and three bands so we can still visualize and reason about it in 3D space. In real-world remote sensing, however, we often work with millions of pixels and possibly hundreds of spectral bands, making direct visualization impossible. That‚Äôs why it‚Äôs helpful to build an intuitive understanding using simple 3D examples first. Once we grasp the core ideas in a space we can see, it becomes much easier to generalize those concepts mathematically to higher-dimensional data where we rely entirely on abstract reasoning and computation.</p> <p>Personally, I find that understanding the data representation used in each analytical method helps us think more clearly about how the method works, and also gives us the ability to adjust our analysis according to our needs. Instead of just following a fixed workflow, we can be more intentional in how we prepare and structure our data. This flexibility becomes especially important as we deal with more complex remote sensing problems, where no single representation fits all use cases.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="machine_learning,"/><category term="image_analysis"/><summary type="html"><![CDATA[A practical overview of image data structures and how they support different analytical methods.]]></summary></entry><entry><title type="html">Revisiting NDFI: A Flexible Index for Forest Monitoring and Beyond</title><link href="https://nasirlukman.github.io/blog/2025/ndfi/" rel="alternate" type="text/html" title="Revisiting NDFI: A Flexible Index for Forest Monitoring and Beyond"/><published>2025-06-28T23:36:10+00:00</published><updated>2025-06-28T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/ndfi</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/ndfi/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The <a href="https://registry.verra.org/app/projectDetail/VCS/5085">Tond Tenga Project</a> in Burkina Faso was the <a href="https://esgnews.com/verra-registers-first-carbon-project-under-icvcm-approved-methodology/">first</a> Verified Carbon Standard (VCS) project. As part of their biomass stocking index methodology, they use the Normalized Difference Fraction Index (NDFI), calculated from Landsat 8 and 9 Operational Land Imager (OLI) scenes. This catch my interest and motivated me to revisit the method. In this blog, I‚Äôll write down my notes and key takeaways, both for my own reference and for others who want to understand the core ideas behind the NDFI.</p> <p>At first glance, the name sounds similar to other well-known remote sensing indices like NDVI or NDWI. But unlike those indices, NDFI isn‚Äôt a simple equation applied to standard satellite bands. In fact, I think NDFI adaptation in remote sensing should be seen more as a general concept or approach in feature engineering, rather than a fixed index. Let‚Äôs unpack that in more detail.</p> <h1 id="a-quick-look-at-spectral-mixture-analysis-sma">A Quick Look at Spectral Mixture Analysis (SMA)</h1> <p>NDFI was originally developed to map forest canopy damage caused by selective logging and fires in the Amazon, introduced by <a href="https://www.sciencedirect.com/science/article/abs/pii/S0034425705002385">Souza et al. (2005)</a>. It‚Äôs based on a method called spectral mixture analysis (SMA), or spectral unmixing. It is something I‚Äôve write multiple time in this blog before. But for completeness, let‚Äôs go over a quick refresher.</p> <p>SMA starts from a classic remote sensing problem: a single pixel often contains multiple objects. For example, take the pixel shown in the image below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_mixture_example-480.webp 480w,/assets/img/post_12_mixture_example-800.webp 800w,/assets/img/post_12_mixture_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_mixture_example.png" class="img-fluid rounded" width="100%" height="auto" alt="Ilustration of how inside one pixel can consist of multiple different object of interest. And how the light reflection signal taht is captured by the satellite sensor is a mixed of each individual object reflectance." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Ilustration of how one pixel can consist of multiple object of interest and what the satelite captured is actually a mixed reflectance signal of each individual object. </div> <p>Within that one 10 x 10 m pixel, we have two tree crowns, some grassy ground, and part of a dirt road. In SMA, these individual components are called endmembers. From spectroscopy, we know that different materials reflect and absorb light in different ways, this behavior across wavelengths is what we call a spectral signature.</p> <p>Trees, grass, and soil each have their own characteristic spectral signatures. In multispectral or hyperspectral images, these differences let us distinguish one endmember from another. But when several endmember are mixed within a single pixel, the resulting spectral signal is a mixed spectrum, and that makes it harder to classify the pixel using a simple ‚Äúmatch-the-spectrum‚Äù approach.</p> <p>SMA becomes useful for two main reasons. First, because a mixed pixel hides the original spectral signature of each object, it becomes hard to classify each pixel directly using traditional approaches. SMA helps us make sense of that mix. Second, sometimes we want to get more detailed information than what the resolution of the image allows. For example, we might want to estimate the tree canopy cover inside a single pixel, even if that pixel covers 10 or 30 meters on the ground. SMA lets us break down that pixel and estimate the proportion of each endmembers inside it.</p> <p>SMA typically assumes a linear mixing model, where each endmember‚Äôs spectral signature is multiplied by its area fraction, and then summed. In matrix notation we can write it as:</p> \[\mathbf{y} = \mathbf{E} \mathbf{a}\] <p>Where \(\mathbf{y}\) is the mixed spectra that the sensor capture, \(\mathbf{E}\) is the endmember spectra matrix where each vertical component represent a single endmemebr spectra, and \(\mathbf{a}\) is the area fraction vector which is summed to one.</p> <p>In most SMA workflows, we assume that the endmembers spectra \(\mathbf{E}\) is known. We won‚Äôt go into how to extract the endmembers here, but common approaches include using spectral libraries, manually selecting the purest pixels, or applying automated methods like NFINDR, which I‚Äôve explained in more detail <a href="https://nasirlukman.github.io/blog/2024/nfindr/">here</a>.</p> <p>The main goal is to solve for the vector \(\mathbf{a}\), which gives us the area fraction of each endmembers inside the pixel. This is usually done by solving a constrained optimization problem (non-negative, sum-to-one constraints). You can refer to the equation I used in my earlier post <a href="https://nasirlukman.github.io/blog/2024/distance/">here</a>.</p> <p>Now, is the linear mixing assumption reasonable? This assumption holds true if we assume a completely flat and smooth surface as ilustrated in the image below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_simple_mixture-480.webp 480w,/assets/img/post_12_simple_mixture-800.webp 800w,/assets/img/post_12_simple_mixture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_simple_mixture.png" class="img-fluid rounded" width="100%" height="auto" alt="Ilustration of the surface assumption where linear mixing model is true. In a completely flat and smooth surface, each light rays will only hit each object exactly once before it reflected and captured by the sensor. hence what the sensor capture is simply a linear combination of each object spectral signatured weighted by its area proportion." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Illustration of flat and smooth surface conditions where linear mixing holds true. </div> <p>In this scenario, each light ray from the sun (\(\mathbf{R}_0\)) interacts with only one object before reflecting into the sensor. In this case both \(\mathbf{R}_1\) and \(\mathbf{R}_2\) are pure reflectance (endmember‚Äôs spectral signature) of object 1 and object 2, respectively. What the sensor receives is simply a linear combination of these reflectances weighted by their respective area fractions.</p> <p>However, real-world surfaces are obviously more complex. For example, consider the next image, which better reflects a realistic, vegetated surface:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_complex_mixture-480.webp 480w,/assets/img/post_12_complex_mixture-800.webp 800w,/assets/img/post_12_complex_mixture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_complex_mixture.png" class="img-fluid rounded" width="100%" height="auto" alt="More realistic surface condition of vegetated earth surface. In this case each light ray hit multiple object before reflected out to the sensor. This complex interaction between light and multiple object casuing the mixture to be defiated from the linear model." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. A more realistic illustration of a vegetated earth surface </div> <p>Due to the complex condition of the real surface, light doesn‚Äôt just bounce off one object. It often reflects multiple times between different objects before reaching the sensor. For instance, in the image above, \(\mathbf{R}_1\) is the normal reflectance from the ground, this will be equal to the ground spectral signature. Then we have \(\mathbf{R}_2\), which is the reflectance from the tree trunk. But here, the light hitting the trunk isn‚Äôt direct sunlight anymore, it is already been reflected off the ground. So if we have a reference endmember spectrum for tree bark, it will not be equal to \(\mathbf{R}_2\), because endmember spectrum was measured assuming direct sunlight as the source. Finally, \(\mathbf{R}_3\) becomes even more complex, as it involves multiple reflections of light that already reflected multiple times. These multi-bounce interactions are why the signal the sensor receives doesn‚Äôt follow a simple linear mixture of the endmembers spectra. The assumption breaks down when multiple scattering effects become significant.</p> <p>I bring this up because I think it is important to be aware of the limitation of this approach and to acknowledge that there are methods that try to model this more realistically. For example, <a href="https://link.springer.com/chapter/10.1007/978-3-319-04126-1_19">bilinear mixing models</a> have been shown to perform better in vegetated areas. I also discussed a more physics-based nonlinear model in my earlier research on <a href="https://nasirlukman.github.io/blog/2024/bayes-unmixing/">mineral mixtures</a>.</p> <p>That said, modeling the precise interaction of light and matter on Earth‚Äôs surface is, for most practical purposes, nearly impossible. And once you go down that path, it‚Äôs easy to over-engineer the problem and fall into a rabbit hole: adding tons of complexity for only marginal performance gains.</p> <p>It helps to remember that each time light reflects, it loses intensity. So usually, the strongest contribution to the final signal is from the first object the light hits. For a lot of real-world remote sensing tasks, the linear model gives a good enough approximation. If it doesn‚Äôt, then it‚Äôs time to explore something more complex.</p> <h1 id="ndfi">NDFI</h1> <p>After a long (but necessary) introduction about SMA, now it‚Äôs time to talk about NDFI itself.</p> <p>In the original paper, the authors used Green Vegetation (GV), Non-Photosynthetic Vegetation (NPV), Soil, and Shadow as their endmembers. Since their goal was to detect selectively logged and burned forest, these were the classes they considered useful. The underlying idea is that undisturbed forest has a high proportion of GV, while disturbed forest has more NPV and Soil. So in the original implementation, NDFI grouped all green vegetation into a single GV class.</p> <p>However, when we apply regular spectral unmixing in forested areas, we run into a common problem: shadows. Forest canopies often cast strong shadows, which can dominate parts of the spectral signal and throw off the GV proportion. To correct for this, the original method normalized the GV fraction by the proportion of non-shadow in the pixel:</p> \[GV_{\text{Shade}} = \frac{GV}{1 - \text{Shade}}\] <p>After this correction, they calculated the normalized difference between the corrected GV and the combined NPV + Soil fraction using this equation:</p> \[\text{NDFI} = \frac{GV_{\text{shade}} - (NPV + Soil)}{GV_{\text{shade}} + NPV + Soil}\] <p>This simple adjustment makes a big difference. By correcting GV with shadow and structuring the index around the key land cover fractions, NDFI performs very well for what it was originally designed to do.</p> <p>However, if we want to use NDFI for a different purpose, for example as a predictor for above-ground biomass (ABG), then a critical adaptation of the index may be useful. Intuitively, I think separating trees from grass or shrubs would be helpful, since they have very different biomass, even though both are green vegetation.</p> <p>The main concept of NDFI still holds. What changes is the definition of the endmembers and how we combine them in the index. For instance, in a biomass mapping application, we may want to compute the proportion of tree cover relative to all other land cover types (grass, shrub, NPV, soil, etc). That way, NDFI becomes more aligned with the information we care about.</p> <p>And of course, endmember selection is flexible. Different ecoregions may require different class definitions depending on which surface types are important‚Äîor which ones can be safely ignored.</p> <h1 id="example-ndfi-in-tropical-forest-using-sentinel-2">Example: NDFI in Tropical Forest Using Sentinel-2</h1> <p>Let‚Äôs use Sentinel-2 imagery to look at a tropical rainforest area in Borneo as an example. Below is the RGB image of the area, along with the NDFI result. The NDFI here is computed using Tree, Grass, Soil, and Shadow as the endmembers. The endmembers spectra were extracted manually from the image.</p> <p>There are some buildings visible, but I didn‚Äôt include them as endmembers since they‚Äôre irrelevant for this analysis. I also couldn‚Äôt find a convincing candidate for NPV in this area, so I excluded it. Therefore, the NDFI in this example is calculated by comparing the shadow-corrected Tree fraction with the sum of Grass and Soil fractions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_rgb_and_ndfi-480.webp 480w,/assets/img/post_12_rgb_and_ndfi-800.webp 800w,/assets/img/post_12_rgb_and_ndfi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_rgb_and_ndfi.png" class="img-fluid rounded" width="100%" height="auto" alt="Iamge show the RGB Composite of Sentinel 1 Imagery in a Tropical Forest of Borneo on the left, and the NDFI result showing fraction of shadow corrected tree cover compared to soil and grass on the right" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Sentinel RGB Image on the left, and the NDFI result on the right. </div> <p>A high NDFI value (shown in red) indicates a higher proportion of tree canopy inside a pixel, which usually suggests denser forest. Areas in yellow or orange still have tree canopy dominance, but are less dense. Regions in green to blue have minimal or no tree cover.</p> <p>Even visually, we can already see that this image helps distinguish dense forest, which likely has higher AGB, from sparser or degraded forest. In the context of a stocking index, the next step would be to explore the relationship between this NDFI output and AGB modeled from field measurements.</p> <p>It‚Äôs also important to point out that generating NDFI requires us to first compute the fractional abundance of each endmember from the spectral unmixing. This intermediate result can be useful on its own. For instance, the Soil fraction alone is quite effective at highlighting small roads, which can be a valuable input layer for downstream tasks like road detection using deep learning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_soil_fractions-480.webp 480w,/assets/img/post_12_soil_fractions-800.webp 800w,/assets/img/post_12_soil_fractions-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_soil_fractions.png" class="img-fluid rounded" width="100%" height="auto" alt="Spectral unmixing result showing the fraction of soil area inside every pixel of the image, this is very usefull for example for input for road detection model using deep learning." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. Soil fraction as an intermediate result in the NDFI computation. This image is also useful if the goal is to detect small dirt roads, for example. Notice the small dirt road to the west of the main road, previously hardly visible in the original RGB image. </div> <p>And for standard visualization purposes, it‚Äôs also useful to combine the fraction layers into a false color composite to visualize how different land cover types are distributed spatially.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_12_false_color-480.webp 480w,/assets/img/post_12_false_color-800.webp 800w,/assets/img/post_12_false_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_12_false_color.png" class="img-fluid rounded" width="100%" height="auto" alt="More realistic surface condition of vegetated earth surface. In this case each light ray hit multiple object before reflected out to the sensor. This complex interaction between light and multiple object casuing the mixture to be defiated from the linear model." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. False color composite combining the soil, tree, and grass fractions. This visualization helps make the spatial distribution of land cover types easier to interpret for human analysis. </div> <h1 id="my-takeaway">My Takeaway</h1> <p>I was doing a quick research about above-ground biomass when I stumbled back upon this method. While thinking more carefully about it, and while writing this post, I realized a few important general takeaways about doing remote sensing analysis.</p> <p>First, it‚Äôs tempting to just take some well-known equation, apply it to our data, and call it a day. On the other extreme, we might overthink the physics and math behind the problem, end up overengineering the solution, and spend a lot of time, effort, and money, only to gain marginal improvements in the final result.</p> <p>Ideally, we should aim for something in between. We invest time early on to build a solid understanding of the fundamentals, clarify the real goal of the analysis, and consider the available resources. Then we design a solution that balances practicality, accuracy, and efficiency.</p> <p>In the end, good remote sensing work is not just about applying the right method, but about applying the right level of thinking about the goal, the problem, the solution, and the constraint.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="spectral_unmixing,"/><category term="biomass,"/><category term="forest,"/><category term="environment,"/><category term="multispectral"/><summary type="html"><![CDATA[A practical dive into the Normalized Difference Fraction Index (NDFI), its origins in forest degradation monitoring, a deeper understanding on its foundation, and how we can adapt it for different goals like above-ground biomass estimation.]]></summary></entry><entry><title type="html">Mapping Lithium Bearing Mineral in Granite Core Sample with Hyperspectral Imaging</title><link href="https://nasirlukman.github.io/blog/2025/hyperspectral-lithium-granite-core/" rel="alternate" type="text/html" title="Mapping Lithium Bearing Mineral in Granite Core Sample with Hyperspectral Imaging"/><published>2025-05-03T23:36:10+00:00</published><updated>2025-05-03T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/hyperspectral-lithium-granite-core</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/hyperspectral-lithium-granite-core/"><![CDATA[<p>Detecting lithium in rocks is a known challenge. Standard tools like handheld or desktop XRF can‚Äôt pick it up due to lithium‚Äôs low atomic number (Z=3). Advanced techniques such as ICP-MS, LA-ICP-MS, and LIBS can measure lithium, but they come with trade-offs: they‚Äôre expensive, time-consuming, and at least mildly destructive.</p> <p>At exploration stage, the focus tend to shift to lithium host minerals rahter than the lithium itself, especially spodumene (pyroxene group), lepidolite (mica group), and cookeite (chlorite group). These minerals are more practical to detect using common analytical tools such as XRD, Raman spectroscopy, or reflectance spectroscopy like ASD and Hyperspectral Imaging (HSI). While XRD and Raman have their strengths, HSI offers unique advantages: it‚Äôs non-destructive, spatially continuous (100‚Äì500 Œºm resolution), and scalable‚Äîfrom thin sections to core samples, outcrops, and even satellite scenes.</p> <p>In this post, I‚Äôll show a case study using hyperspectral imaging on a small section of a lithium-bearing granite core sample from Devon, UK.</p> <h2 id="spectral-signatures">Spectral Signatures</h2> <p>distinguish between different white mica species, as only lepidolite contains lithium. White micas exhibit absorption features near 1400 nm, 1900 nm, and especially around 2200 nm due to OH vibrational overtones. Differentiating between species requires detecting subtle shifts in the ~2200 nm feature.</p> <p>Muscovite, which contains Al-OH bonds, shows an absorption feature near 2206 nm. When Al is substituted by other cations like Fe, Mg, or Li, the position of this feature shifts. Specifically, Fe- and Mg-rich white micas such as phengite shift the feature to longer wavelengths (~2230 nm), while Li-rich micas like lepidolite shift it to shorter wavelengths (~2195 nm). In this rock sample, we observe this feature ranging from ~2195 nm to ~2208 nm, indicating a compositional transition from lepidolite to muscovite.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_10_lepidolite_muscovite_spectra-480.webp 480w,/assets/img/post_10_lepidolite_muscovite_spectra-800.webp 800w,/assets/img/post_10_lepidolite_muscovite_spectra-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_10_lepidolite_muscovite_spectra.png" class="img-fluid rounded" width="100%" height="auto" alt="Spectral signature comparison between lepidolite and muscovite in a lithium-rich granite sample. The plot highlights the subtle shift in the 2200 nm absorption feature, with lepidolite showing a feature near 2195 nm (shorter wavelength) and muscovite at 2206 nm (longer wavelength). This shift is key to differentiating lithium-bearing minerals from other white mica species using hyperspectral imaging." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Difference between lepidolite and muscovite spectra. </div> <h2 id="minimum-wavelength-mapping">Minimum Wavelength Mapping</h2> <p>To visualize subtle shifts in absorption features, we use Minimum Wavelength Mapper. Generally this method can be devided into four steps.</p> <ol> <li>Detect the deepest absorption feature within a chosen wavelength range.</li> <li>Fit a 2nd-order polynomial around the feature to interpolate the true minimum (often sub-band resolution).</li> <li>Extract the interpolated wavelength position and absorption depth.</li> <li>Map them using hue (wavelength) and brightness (depth).</li> </ol> <p>For this analysis I am using an open source software called <a href="https://zenodo.org/records/14335092">Hyppy</a> which are developed by the ITC Earth Science Labs in University of Twente. The result are shown in the image below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_10_minimum_wavelength_mapper-480.webp 480w,/assets/img/post_10_minimum_wavelength_mapper-800.webp 800w,/assets/img/post_10_minimum_wavelength_mapper-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_10_minimum_wavelength_mapper.png" class="img-fluid rounded" width="100%" height="auto" alt="Minimum Wavelength Map of a lithium-bearing granite core sample showing the spatial distribution of the deepest absorption feature around 2200 nm. The image is color-coded to represent wavelength position, with blue indicating shorter wavelengths and purple for longer wavelengths. The brightness indicates absorption depth, with dark pixels showing absence of white mica minerals and lighter pixels representing areas where they are present. The map shows a transition from muscovite at the margins of the granite to lepidolite in the core." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Minimum Wavelenth Map of the rock sample showing the gradual change from lepidolite to muscovite. </div> <p>In this image, the wavelength position of the deepest absorption feature is represented by hue, where the color blue indicating shorter wavelengths and purple indicating longer wavelengths. The depth of the feature is shown as brightness. Dark or black pixels correspond to areas where the OH feature is absent, meaning no white mica minerals are present. Several important observations can be made from this visualization. First, the occurrence of white mica minerals appears to be confined to the main body of the granite, and they are largely absent in both the surrounding veins and the host rock. Second, there is a clear spatial gradient in the wavelength position of the deepest absorption feature, shifting from longer wavelengths near the contact zone between the granite and the host rock, to shorter wavelengths deeper within the granite body. This pattern reflects a compositional transition in the white mica group, from muscovite at the margins to lepidolite in the granite core. This suggests that the original granite magma was already enriched in lithium, and interaction with the surrounding sedimentary rocks likely introduced additional aluminum‚Äîand possibly iron and magnesium‚Äîsubstituting for lithium in the octahedral sheets of the mica structure.</p> <h2 id="classification">Classification</h2> <p>While minimum wavelength mapping offers rich insight, classifications help simplify the output. I applied a decision tree classifier to assign each pixel to one of 10 classes based on the absorption wavelength and depth values.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_10_image_classification-480.webp 480w,/assets/img/post_10_image_classification-800.webp 800w,/assets/img/post_10_image_classification-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_10_image_classification.png" class="img-fluid rounded" width="100%" height="auto" alt="Decision Tree Classifier result for the lithium-bearing granite core sample, showing 10 different mineral groups based on absorption wavelength and depth values. The classification identifies various minerals, including different species of white mica, based on the spectral features detected by hyperspectral imaging. This map helps visualize the mineral zoning within the granite, highlighting areas rich in lithium-bearing minerals like lepidolite." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Decision Tree Classifier result of the rock sample showing 10 different mineral group in the rock sample. </div> <p>This case study highlights how HSI is a powerful tool for identifying lithium-bearing minerals through subtle spectral variations. By focusing on the spectral behavior of white micas, particularly shifts in the ~2200 nm absorption feature, we can non-destructively infer mineralogical variations linked to lithium content. The spatial resolution and continuous coverage offered by hyperspectral data enable detailed mapping of mineral zoning within core samples, providing valuable insight into the geological processes at play.</p> <p>The spatial distribution of white mica species is valuable beyond lithium exploration, as different white micas can be associated with distinct temperature and geochemical conditions in hydrothermal systems, making them effective vectors toward ore bodies. One of the key advantages of HSI is its scalability: the same spectral analysis applied to core samples can be extended to outcrops and even regional scales using airborne or satellite platforms.</p>]]></content><author><name></name></author><category term="spectral_geology,"/><category term="mineral,"/><category term="data_science,"/><category term="geology,"/><category term="hyperspectral,"/><category term="core_scanning"/><summary type="html"><![CDATA[Using Minimum Wavelength Mapper and Decision Tree Classifier to analyze hyperspectral image of lithium bearing granite core sample.]]></summary></entry><entry><title type="html">Multi Sensor Object Based Image Classification for Land Use/Land Cover Analysis</title><link href="https://nasirlukman.github.io/blog/2025/OBIA-LULC/" rel="alternate" type="text/html" title="Multi Sensor Object Based Image Classification for Land Use/Land Cover Analysis"/><published>2025-04-08T23:36:10+00:00</published><updated>2025-04-08T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/OBIA-LULC</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/OBIA-LULC/"><![CDATA[<p>In this post, I will share a Land Use Land Cover (LULC) project I did combining multiple sensors. To tackle the common problem of the salt and pepper effect in pixel-based image classification, an object-based approach was chosen for this task. The steps generally involved stacking images from multiple sensors, performing image segmentation using SNIC, taking the mean value of each band from each segment, and running supervised classification using Random Forest.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_classification%20steps-480.webp 480w,/assets/img/post_9_classification%20steps-800.webp 800w,/assets/img/post_9_classification%20steps-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_classification%20steps.gif" class="img-fluid rounded" width="100%" height="auto" alt="Classification steps showing the process from image stacking, segmentation with SNIC, feature extraction, and supervised classification with Random Forest." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Classification steps. </div> <h2 id="methods">Methods</h2> <p>The land cover classes consist of categories such as forest, shrubs, mangroves, bare land, built-up areas, water, and plantations. Optical and radar images are known to have effective discriminating ability among those classes, which is why we used Landsat-8 and Sentinel-1 sensors. The VIIRS night-time light sensor captures light emissions on the Earth‚Äôs surface, which is useful to detect built-up areas. At this stage, we also included elevation data from SRTM, since it was available.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_raw_data-480.webp 480w,/assets/img/post_9_raw_data-800.webp 800w,/assets/img/post_9_raw_data-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_raw_data.png" class="img-fluid rounded" width="100%" height="auto" alt="Raw satellite images from Landsat-8, Sentinel-1, VIIRS NTL, and SRTM elevation data, serving as inputs for land use classification." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Raw satellites images input. </div> <p>The next step was to extract additional features from the raw data. From the Landsat image, we calculated several spectral indices to highlight different features of the land surface. We also calculated GLCM features to extract textural information. Additionally, we computed band ratios from Sentinel-1 imagery, and we calculated slope from the SRTM elevation data. In total, 27 bands were obtained and prepared as inputs for the classification.</p> <p>Not all of those bands are useful for the classification purpose, so the next step was feature selection to pick only the most important bands for the final classification. Permutation importance was used for this, which also considers simple correlation between bands. Eight bands were selected for the final classification:</p> <ol> <li>Modified Normalized Difference Water Index (MNDWI)</li> <li>VV/VH Sentinel 1 Ratio</li> <li>GLCM Textural Contrast</li> <li>Normalized Difference Vegetation Index (NDVI)</li> <li>VIIRS Night Light Radiance</li> <li>Landsat Band 1</li> <li>Build up Index (BI)</li> </ol> <h2 id="results-and-insight">Results and Insight</h2> <p>The final result is an annual LULC map of the area. Overall accuracy is around 96%, with most of the errors coming from false positives in palm oil plantations. The palm oil plantations in this region are quite heterogeneous in which some areas are densely planted, while others are more spacious. The more spacious ones often appear similar to shrubs or low canopy forests, which causes confusion.</p> <p>The model can then be generalized to different timestamps with roughly the same accuracy (94‚Äì96%).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_results_3_year-480.webp 480w,/assets/img/post_9_results_3_year-800.webp 800w,/assets/img/post_9_results_3_year-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_results_3_year.gif" class="img-fluid rounded" width="100%" height="auto" alt="Final land use/land cover classification result from 2017 to 2024 showing the area‚Äôs land cover change." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Classification Results </div> <p>If we look at the results from 2017 to 2024, we can see some interesting trends:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_LULC_graph-480.webp 480w,/assets/img/post_9_LULC_graph-800.webp 800w,/assets/img/post_9_LULC_graph-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_LULC_graph.png" class="img-fluid rounded" width="100%" height="auto" alt="Time series graph of land use/land cover changes, highlighting trends in various land cover classes from 2017 to 2024." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Time series graph of few LULC class </div> <p>First, there‚Äôs a sharp decline in high-density forest from 2017 to 2019, which then stabilizes. We can also see a slow and steady decline in mangroves. Interestingly, for both of these classes, the land cover changes aren‚Äôt shifting toward human-made landscapes like built-up areas, plantations, or even bare land. Most of the changes are into shrubs. This might be related to more indirect effects of human activity on the environment rather than direct conversion.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_LULC_mangrove-480.webp 480w,/assets/img/post_9_LULC_mangrove-800.webp 800w,/assets/img/post_9_LULC_mangrove-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_LULC_mangrove.png" class="img-fluid rounded" width="100%" height="auto" alt="LULC change showing the slow decrease of mangrove cover from 2017 to 2024." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. LULC change showing slow decrease of mangrove cover from 2017 to 2024. </div> <p>Second, there‚Äôs a quite remarkable increase in low canopy forest. Most of the low canopy forest areas appear to have changed from shrubs. The most significant increase happened between 2019 and 2024, which is the same period when the decline of high canopy forest slowed down or stopped. Both of these phenomena are likely the result of conservation and restoration efforts that are currently ongoing in the region.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_LULC_forest-480.webp 480w,/assets/img/post_9_LULC_forest-800.webp 800w,/assets/img/post_9_LULC_forest-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_LULC_forest.png" class="img-fluid rounded" width="100%" height="auto" alt="LULC change showing the increase of low canopy forest cover from 2017 to 2024, highlighting changes from shrubs." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. LULC change showing increase of low canopy forest cover from 2017 to 2024. </div> <p>The final thing that stands out is the slow and steady increase in built-up areas. Most of the new built-up areas seem related to local community settlements in small towns or villages near riverbanks, and some appear to be plantation housing and factory sites.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_9_LULC_buildup-480.webp 480w,/assets/img/post_9_LULC_buildup-800.webp 800w,/assets/img/post_9_LULC_buildup-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_9_LULC_buildup.png" class="img-fluid rounded" width="100%" height="auto" alt="LULC change showing the slow increase of built-up areas from 2017 to 2024, indicating urban development in riverbank settlements." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. LULC change showing slow increase of build up area from 2017 to 2024. </div> <hr/> <p>This project shows how combining different sensors and using an object-based approach can produce accurate and insightful land cover maps. Beyond just the numbers, the patterns we see from year to year tell a deeper story about environmental change, pressure from human activity, and efforts to restore and protect our landscapes.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="monitoring,"/><category term="environment,"/><category term="forest,"/><category term="data_science,"/><category term="machine_learning,"/><category term="radar,"/><category term="multispectral"/><summary type="html"><![CDATA[Combinining Landsat-8, Sentinel-1, VIIRS NTL, and SRTM for object based image classification using SNIC and Random Forest]]></summary></entry><entry><title type="html">Spatio-Temporal Analysis of Industrial and Artisinal Offshore Tin Mining Vessel</title><link href="https://nasirlukman.github.io/blog/2025/mining-vessel/" rel="alternate" type="text/html" title="Spatio-Temporal Analysis of Industrial and Artisinal Offshore Tin Mining Vessel"/><published>2025-04-04T23:36:10+00:00</published><updated>2025-04-04T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/mining-vessel</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/mining-vessel/"><![CDATA[<p>Most of the tin sold on the international market comes from a small island in Indonesia called Bangka. What‚Äôs less known is that a significant amount of that tin is mined offshore. There are two types of vessels commonly involved in this activity. Large dredger vessels, typically 50 to 120 meters in length, are operated by formal mining companies at industrial scale. At the same time, there are smaller artisanal boats‚Äîusually no more than 10 meters long‚Äîusing simple suction pumps handled by divers and connected to traditional sluice boxes onboard. The later are typically related with illegal mining activities.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_vessel_example-480.webp 480w,/assets/img/post_8_vessel_example-800.webp 800w,/assets/img/post_8_vessel_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_vessel_example.png" class="img-fluid rounded" width="100%" height="auto" alt="Two types of offshore mining vessels: large dredger vessels (metal-bodied, 50-120 meters) and smaller artisanal vessels (wooden, under 10 meters)." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Two types of offshore mining vessels. </div> <h2 id="ship-detection-and-classification">Ship Detection and Classification</h2> <p>To better understand how offshore mining activity changes over time, I‚Äôve been developing a ship detection algorithm tailored to this region. I use Sentinel-1 SAR data, which is particularly suitable for this task. Not only can it see through the frequent cloud cover over the shores of Bangka, but it also does a good job of capturing the contrast between open water and floating objects‚Äîeven relatively small ones.</p> <p>For this study, I focused on the eastern part of Bangka Island.</p> <p>Before doing any detection, I needed a good quality ‚Äòground truth‚Äô data. In this case that meant finding scenes where Sentinel-1 and Sentinel-2 images‚Äîor ideally very high-resolution optical images‚Äîwere available on the same date. This allows for visual confirmation of vessel types and ensures that the training data is as accurate as possible.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_small_vessel_example-480.webp 480w,/assets/img/post_8_small_vessel_example-800.webp 800w,/assets/img/post_8_small_vessel_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_small_vessel_example.png" class="img-fluid rounded" width="100%" height="auto" alt="Worldview, Sentinel-2, and Sentinel-1 imagery taken at the same location with only a few hours apart showing a small artisanal mining vessel" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Worldview, Sentinel-2, and Sentinel-1 imagery taken at the same spot with only few hours apart showing small/artisinal mining vessel. </div> <p>After collecting a good number of examples, I started looking into how the SAR sensor responds to the different types of vessels. It turns out that their radar signatures are quite distinct, especially when you compare the VV and VH polarizations of Sentinel-1.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_training_data_distribution-480.webp 480w,/assets/img/post_8_training_data_distribution-800.webp 800w,/assets/img/post_8_training_data_distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_training_data_distribution.png" class="img-fluid rounded" width="100%" height="auto" alt="Sentinel-1 sensor responses of large (metal) and small (wooden) vessels in both VV and VH polarizations, showing distinct radar signatures." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Sentinel-1 sensor responses of the large and small vessels. </div> <p>The differences in backscatter likely relate to both the size of the vessels and the materials they‚Äôre made of. while large mining vessel are made out of metals, the smaller artisinal mining vessel are made out of wood.</p> <p>For classification, I used an object-based approach. First, I segmented the image, and then applied a support vector machine (SVM) to classify the segmented objects. The training result was promising: a validation accuracy of about 98%.</p> <h2 id="results-and-analysis">Results and Analysis</h2> <p>I deployed the trained model across a time series of Sentinel-1 imagery, covering the period from early 2017 to early 2025. The result is a spatial and temporal map of detected vessels.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_result_stacked-480.webp 480w,/assets/img/post_8_result_stacked-800.webp 800w,/assets/img/post_8_result_stacked-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_result_stacked.png" class="img-fluid rounded" width="100%" height="auto" alt="Object-based classification result showing the detection of large and small mining vessels in the study area." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Object based classification result example. </div> <p>To make sense of these results, I created a time series map animatioin showing the density of both large and small vessels over time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_vessel_density_timelapse-480.webp 480w,/assets/img/post_8_vessel_density_timelapse-800.webp 800w,/assets/img/post_8_vessel_density_timelapse-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_vessel_density_timelapse.gif" class="img-fluid rounded" width="100%" height="auto" alt="Time series animation showing the density of large and small vessels over time, illustrating the changes in offshore mining activity." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. Timseries animation of the large and small vessels density. </div> <p>And add a plot to tracks the number of vessels by type at each timestamp.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_timeseries_plot-480.webp 480w,/assets/img/post_8_timeseries_plot-800.webp 800w,/assets/img/post_8_timeseries_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_timeseries_plot.png" class="img-fluid rounded" width="100%" height="auto" alt="Timeseries plot showing the count of large and small vessels detected at each timestamp, indicating fluctuations in mining activity." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. Timeserise plot of ship counts for each class. </div> <p>Since it is quite difficult to extract meaningful infomration from this plot alone, I used seasonal decomposition to separate long-term trends from seasonal cycles.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_seasonal_decomposition-480.webp 480w,/assets/img/post_8_seasonal_decomposition-800.webp 800w,/assets/img/post_8_seasonal_decomposition-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_seasonal_decomposition.png" class="img-fluid rounded" width="100%" height="auto" alt="Seasonal decomposition of the time series result, separating long-term trends and seasonal cycles in mining vessel activity" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. Seasonal decomposition of the timeseris result </div> <p>Both vessel types show the same general long term trends, and both have a sharp drop in activity around late 2020‚Äîmost likely a result of the COVID-19 outbreak. The seasonal patterns are also quite interesting. Smaller vessels tend to be most active between January and March, with a noticeable peak in January. Larger vessels, on the other hand, see more activity in March to May and again in October and November.</p> <h2 id="notes-and-consideration">Notes and Consideration</h2> <p>Since this is a mining area, most of the ships present are a mining vessel. However, other ship types are present as well, i.e. cargo ships and fishing boats. Cargo vessels tend to be large and metal-bodied, which makes them look similar to large mining dredgers in SAR imagery. On the other hand, traditional fishing boats are smaller and usually built from wood, making them potential look-alikes for artisanal mining vessels.</p> <p>Ideally, these look-alike vessels would be included in the training data to reduce false positives. However, collecting reliable ground truth for them is difficult‚Äîespecially because identifying vessel types often relies on cloud-free optical imagery taken at the same time as the Sentinel-1 scenes. This overlap is rare, making it harder to build a well-labeled dataset and limiting the number of high-quality training samples available for the model.</p> <p>One interesting observation is that mining vessels often operate in clusters. This spatial pattern could be a useful cue for distinguishing them from other types of vessels‚Äîespecially when radar backscatter alone isn‚Äôt enough.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_8_hot_spots-480.webp 480w,/assets/img/post_8_hot_spots-800.webp 800w,/assets/img/post_8_hot_spots-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_8_hot_spots.png" class="img-fluid rounded" width="100%" height="auto" alt="All ships detected in the study area, with green boxes highlighting hotspots where mining vessels typically operate in clusters." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8. all ships detected in the study area. Green box highlight hot-spots for ships which is a typical pattern for mining vessels. </div> <h2 id="to-wrap-up">To Wrap Up</h2> <p>Offshore tin mining around Bangka is a complex activity, spanning from industrial-scale operations to small artisanal efforts. Continuous monitoring of these activities could helps bring transparency to an industry that often operates in remote and loosely regulated areas. Using freely available satellite data, especially Sentinel-1 SAR, offers a promising way to monitor these activities systematically over time. While there are still challenges such as distinguishing mining vessels from other ships or dealing with limited training data, the initial results shows a strong potential to reveal a meaningful patterns.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="monitoring,"/><category term="mining,"/><category term="security,"/><category term="data_science,"/><category term="machine_learning,"/><category term="radar,"/><category term="GIS"/><summary type="html"><![CDATA[Object-based image classification of mining vessels using Sentinel-1 timeseries.]]></summary></entry><entry><title type="html">Sentinel 1 Change Detection for Forest Monitoring</title><link href="https://nasirlukman.github.io/blog/2025/sentinel1-change-detection/" rel="alternate" type="text/html" title="Sentinel 1 Change Detection for Forest Monitoring"/><published>2025-03-30T23:36:10+00:00</published><updated>2025-03-30T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/sentinel1-change-detection</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/sentinel1-change-detection/"><![CDATA[<p>Last week, I was trying to build a forest monitoring system using Sentinel-1 data. The idea of using radar data instead of optical data is that the climate of most tropical forest areas in the world is characterized by frequent and persistent rainfall, so the cloud cover tends to be high throughout the year. For example, the <a href="https://glad.earthengine.app/view/global-forest-change">Hansen deforestation dataset</a> is a very good-quality global deforestation dataset built using Landsat optical satellite data. The dataset is annual from 2000 to 2023 (last time I checked). I can imagine it would be very hard, and even impossible in some parts of the world, to build a monthly deforestation layer using the same approach due to cloud cover. Unlike optical sensor, radar signal have the ability to penetrate colud cover, making it a more reliable for monitoring in high cloud cover areas.</p> <p>The Sentinel-1 forest monitoring system I built is based on change detection at its core. It simply involves subtracting images from \(\mathbf{t_1}\) and \(\mathbf{t_2}\). However, due to the high noise of radar images and the different atmospheric conditions in \(\mathbf{t_1}\) and \(\mathbf{t_2}\), some care needs to be taken when pre-processing each image. That, and some additional image analysis tricks and magic!</p> <p>Generally, the steps are as follows:</p> <ol> <li> <p><em>Filtering Sentinel-1 Metadata</em>: We filter the metadata to only get images with identical orbital passes and numbers. Radar images are especially sensitive to viewing geometry, so we need to make sure that the viewing geometry of each image is as similar as possible.</p> </li> <li> <p><em>Speckle Filtering</em>: We use both spatial and temporal axes to do speckle filtering using a three-dimensional kernel.</p> </li> <li> <p><em>Additional Normalization and Time-Series Aggregation</em>: These steps help minimize changes that are not related to forest cover change.</p> </li> <li> <p><em>Forest Mask</em> (ideally): We also need to incorporate a forest mask to remove any changes that are not related to the forest (which I didn‚Äôt include this time).</p> </li> <li> <p><em>Change Detection</em>: Subtracting images in concecutive time periods and applying threshold value to filter out minor changes due to noises.</p> </li> </ol> <p>Below are examples of the results of this method compared to the Hansen layer for some forest areas in Borneo, which have been converted into palm oil plantations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_7_result_comparison-480.webp 480w,/assets/img/post_7_result_comparison-800.webp 800w,/assets/img/post_7_result_comparison-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_7_result_comparison.png" class="img-fluid rounded" width="100%" height="auto" alt="Side-by-side comparison between the Hansen deforestation layer (annual) and the Sentinel-1 change detection layer (monthly) for a forest area in Borneo, showing a large deforestation patch in 2024 not recorded in the Hansen dataset." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Result comparison between Hansen deforestation layer and Sentinel 1 change detection layer on the same area in Borneo. </div> <p>The main difference between the two results is the big patch of deforestation in the top-right layer, which was not recorded in the Hansen layer. The Hansen dataset I acquired from Earth Engine only goes up to 2023, and those deforestations are happening in 2024. The most important difference, of course, is the time frequency. My approach records monthly changes, so if we zoom into the top-left patch of the image as an example, we can see the detail of the deforestation progression throughout the months of the year. In this particular case, the palm oil plantation seems to expanded towards the north with pace of 50 Ha/month for a time period of 22 months from June 2018 until March 2020. This example shows the potential of using this type of monthly information to build an early warning system to monitor forest area throughout the globe.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_7_zoomed_in-480.webp 480w,/assets/img/post_7_zoomed_in-800.webp 800w,/assets/img/post_7_zoomed_in-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_7_zoomed_in.png" class="img-fluid rounded" width="100%" height="auto" alt="Zoomed-in view of the top-left deforested area in the Sentinel-1 change detection result, highlighting the detailed progression of deforestation in a palm oil plantation from June 2018 to March 2020, with an expansion rate of 50 hectares per month." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Zoomed in result at the top-left deforested patch of the image. </div> <p>Another potential issue (or maybe a potential feature) of this approach is that, our measurement is essentially a proxy for forest canopy density change. This means it is not only sensitive to deforestation but also to disturbance. Unfortunately, without ground truth, it is difficult to make sure whether these patches are indeed related to actual disturbance events or an artifact of sensor noise. My best bet is that it is currently the combination of both. If we want to exclude these small changese patches we can adjust the threshold, and add aditional post-processing step such as majority kernel. It is very difficult to find an optimum threshold for a very large area (i.e. the whole planet) so some trade-off decision need to be made.</p> <p>Note: There are similar datasets that also use Sentinel-1 to monitor forest changes monthly, such as <a href="https://data.globalforestwatch.org/datasets/gfw::deforestation-alerts-radd/about">RADD Alerts</a>. I compared my result with RADD Alerts, and it produced almost identical results. The advantage of using your own method instead of relying on an existing global dataset is the flexibility to adjust parameters if necessary.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="monitoring,"/><category term="data_science,"/><category term="radar,"/><category term="environment,"/><category term="forest"/><summary type="html"><![CDATA[Using Sentinel-1 data to detect monthly disturbance of forest canopy]]></summary></entry><entry><title type="html">Sentinel-2 Bare Earth Mosaics</title><link href="https://nasirlukman.github.io/blog/2025/bare-earth/" rel="alternate" type="text/html" title="Sentinel-2 Bare Earth Mosaics"/><published>2025-02-19T23:36:10+00:00</published><updated>2025-02-19T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/bare-earth</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/bare-earth/"><![CDATA[<p>As a tropical country, Indonesia faces challenges in using optical remote sensing data for soil and geological analysis due to extensive vegetation cover. According to the Indonesia Land Use/Land Cover (LULC) Map of 2019, only 1.8% of the land is classified as bare land. However, a significant portion of vegetated area (38.2%) consists of agricultural land. During land use changes, particularly in the early stages of agricultural development, land clearing exposes the bare soil. Additionally, many agricultural areas follow cultivation cycles, periodically revealing the soil beneath.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_6_landuse_chart-480.webp 480w,/assets/img/post_6_landuse_chart-800.webp 800w,/assets/img/post_6_landuse_chart-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_6_landuse_chart.png" class="img-fluid rounded" width="100%" height="auto" alt="Donut chart showing the percentage of land use types in Indonesia, reclassified from the Indonesia Land Use/Land Cover (LULC) Map 2019, highlighting bare land, agriculture, forest, and other land cover categories." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Indonesia Land Use percetage, reclassified from Indonesia LULC Map 2019. </div> <p>By leveraging dense time-series satellite images, we can generate a bare earth mosaic, which combines pixels from different acquisition times to maximize the exposure of the ground surface. <a href="https://www.eftf.ga.gov.au/case-study/sentinel-2-reveals-barest-earth">Geoscience Australia</a> has created such maps for the entire continent using Landsat and Sentinel-2 images. Inspired by this, I recently experimented with a method to build a bare earth mosaic for Indonesia using Google Earth Engine.</p> <h2 id="approach">Approach</h2> <p>This approach selects only the driest, non-vegetated pixels from the time-series collection while masking out permanently vegetated, wet, and built-up areas. The process involves:</p> <ol> <li> <p>Cloud Masking: Cloudmasking using <a href="https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless">s2Cloudless</a></p> </li> <li> <p>Pixel Filtering: Using various spectral indices for vegetation, water, and soil to exclude unwanted pixels.</p> </li> <li> <p>Geometric Median Computation: Calculating the geometric median of the remaining pixels to minimize noise and cloud contamination.</p> </li> <li> <p>Final Masking: Removing residual built-up areas using LULC maps.</p> </li> </ol> <p>The resulting image is a bare earth mosaic, highlighting exposed soil and geological features. For visualization purpose, masked vegetation, water, and built-up pixels can be reintroduced.</p> <h2 id="comparison-with-standard-cloudless-mosaics">Comparison with Standard Cloudless Mosaics</h2> <p>The images below compare a standard cloudless mosaic with a bare earth mosaic derived from Sentinel-2 images. The latter effectively selects the barest pixels within agricultural areas and built-up zones.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_6_agriculture-480.webp 480w,/assets/img/post_6_agriculture-800.webp 800w,/assets/img/post_6_agriculture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_6_agriculture.png" class="img-fluid rounded" width="100%" height="auto" alt="Side-by-side comparison of a standard cloudless mosaic and a bare earth mosaic over agricultural land, showcasing how the bare earth mosaic better highlights exposed soil within agricultural areas." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Comparison between cloudless mosaic and bare earth mosaic in agriculural land. </div> <p>This method also reveals earth surfaces in permanently forested areas, such as landslide-exposed soil and sandbars in rivers during droughts or after heavy rainfall, when sediment is deposited.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_6_river_and_landslide-480.webp 480w,/assets/img/post_6_river_and_landslide-800.webp 800w,/assets/img/post_6_river_and_landslide-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_6_river_and_landslide.png" class="img-fluid rounded" width="100%" height="auto" alt="Side-by-side comparison of a cloudless mosaic and a bare earth mosaic, showing additional features like exposed riverbeds and soil in landslide areas, visible in the bare earth mosaic." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Comparison between cloudless mosaic and bare earth mosaic showing additional in river and landslide. </div> <h2 id="applications-and-challenges">Applications and Challenges</h2> <p>For further geological and soil analysis, it is best to use the masked image. For example in the image below, a simple false color-composite already show:</p> <ol> <li> <p>Soil composition variations in southern hills (greensih color), with some material transported north via river systems.</p> </li> <li> <p>Differences between eastern (yellowish) and western plains (orange).</p> </li> <li> <p>Distinct compositions within mining pits (very bright color).</p> </li> </ol> <p>This color composite highlight the potential of bare earth moscais for geological/soil analysis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_6_false_color-480.webp 480w,/assets/img/post_6_false_color-800.webp 800w,/assets/img/post_6_false_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_6_false_color.png" class="img-fluid rounded" width="100%" height="auto" alt="Comparison of a cloudless mosaic, bare earth mosaic, and false color composite of the masked bare earth mosaic, with a focus on identifying soil composition variations, river sediment transport, and distinct material compositions within mining pits." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Cloudless composite, and bare earth composite, and false color of masked bare earth composite for analysis and interpretation. </div> <p>The main challenge in generating a nationwide bare earth composite for Indonesia is scalability. My current approach exceeds Google Earth Engine‚Äôs computational limits when applied at scale. To overcome this, I need to optimize the process or split the computation into hundreds of smaller patches.</p> <p>This is an ongoing experiment, and I hope to refine the method further. If you have suggestions or insights, please feel free to share!</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="soil,"/><category term="geology,"/><category term="data_science,"/><category term="multispectral"/><summary type="html"><![CDATA[First trial to build Sentinel-2 bare earth mosaics for Indonesia in Earth Engine]]></summary></entry><entry><title type="html">Illegal Mining From Space</title><link href="https://nasirlukman.github.io/blog/2025/illegal-mining/" rel="alternate" type="text/html" title="Illegal Mining From Space"/><published>2025-02-09T23:36:10+00:00</published><updated>2025-02-09T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2025/illegal-mining</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2025/illegal-mining/"><![CDATA[<p>High-resolution satellite imagery provides us with global time-series data, which can be applied to various interesting applications. One such application is monitoring mining activities. There are literally thousands of mining sites across the globe, which are inherently dynamic (i.e., they undergo rapid changes over time) and often located in remote areas. This creates significant challenges for effective monitoring.</p> <p>Any serious attempt at a global-scale analysis of these activities requires the use of satellite imagery in some form. Information such as the extent of mining activity, the mining methods used, and the commodities being extracted can be interpreted from satellite data. Additionally, parameters like land-use changes, soil contamination, and surface water pollution can also be analyzed using satellite imagery.</p> <p>Of course, when discussing illegal mining specifically, satellite data alone may not be sufficient to characterize it fully. This is because the legal frameworks governing mining activities are often complex and vary from country to country. However, there are certain indicators that can help identify potential illegal activities. By incorporating additional layers of geospatial data, stronger evidence of illegal mining can be extracted.</p> <h2 id="very-high-resolution-images">Very High-Resolution Images</h2> <p>A wealth of information related to mining activities can be obtained from very high-resolution satellite images. The most obvious is the delineation of mining area boundaries. With time-series images, we can observe the development of mining activities and the areas they impact. Less obvious but equally important information includes the type of commodity being mined and the methods used. For those familiar with the mining industry, such details can often be inferred from satellite imagery alone. For example, coal mining can be identified by the black color of coal at the mining front or in stockpiles. Similarly, nickel and bauxite lateritic deposits often have distinct visual characteristics that are easily recognizable from space.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_vhr_example-480.webp 480w,/assets/img/post_5_vhr_example-800.webp 800w,/assets/img/post_5_vhr_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_vhr_example.png" class="img-fluid rounded" width="100%" height="auto" alt="Satellite imagery from WorldView showing various mining activities in Indonesia, including distinct alluvial gold and tin mining operations that appear informal and lack structured infrastructure." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Different type of mining activity in Indonesia from WorldView imagery. </div> <p>One key indicator of illegal mining that can be extracted from satellite data is the mining method. Mining activities are often categorized as formal or informal. Formal mining is conducted using appropriate tools and is planned and executed by certified engineers, resulting in well-structured mining pits and infrastructure that are clearly visible from space. In contrast, informal mining is often chaotic and sporadic, lacking proper planning and adherence to good mining practices. The latter type is frequently associated with illegal mining activity, although this is not always the case.</p> <p>In Image 1 above, we can clearly see that the alluvial gold and tin mining example can be classified as informal mining. There are no visible mining infrastructures in the area, and the mining pit appears sporadic and abandoned, resembling a pond.</p> <p>While proven to be very useful, a significant drawback of very high-resolution images is that they are mostly not open data, making access to large spatial and temporal-scale datasets prohibitively expensive. For this reason, large-scale studies often rely more on high-resolution images, using only scarce and limited amounts of very high-resolution data for creating training datasets and/or validation.</p> <h2 id="automated-approach">Automated Approach</h2> <p>Given the large spatial and temporal scales involved, especially for time-series analysis on a regional or global level, an automated approach to mining area delineation and classification is essential. As previously mentioned, human experts interpret mining areas based on visual aspects rather than spectral characteristics (though hyperspectral imagery can improve accuracy). These mining areas are primarily recognized by their spatial patterns. Until recently, deep learning models have proven to be the most effective solution for this type of problem.</p> <p>I develop a small scale deep learning model to automate delinitation of artisinal/small-scale gold mining (ASGM) in some portecteed forest area in Indonesia. I used coarser-resolution NICFI satellite data with a pixel size of 5 meters, which is still sufficient for this purpose. Even lower-resolution satellite images, such as those from Sentinel-2 or Landsat, can produce good results for delineation. The training labels for ASGM boundary were manually delineated from very high-resolution WorldView images, which were then transferred to NICFI images from the same month. The model used was a U-Net architecture, trained on 39 tiles of 256x256 pixels, augmented to 195 images through flipping and rotation. Despite the relatively small training dataset, the model performed well in tracking the development of illegal alluvial gold mining in a protected forest in Indonesia.</p> <p>Below are the accuracy assessments from the validation images. The overall accuracy are 98%. Here, I only show a few validation images containing difficult scenery and look-alike features, such as large active channel bars, formal coal mining areas, and deforestation for agriculture. The model correctly labeled most of these as non-artisanal and small-scale gold mining (ASGM). Some notable false positives were detected in the ponds of formal coal mining areas. It is important to note that the current training data contain no examples of other mining types besides ASGM. Given this limitation, the fact that the model does not classify the entire area of coal mining as ASGM is already a strong result. Incorporating additional, more diverse training data would likely improve the model‚Äôs accuracy further.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_unet_validation-480.webp 480w,/assets/img/post_5_unet_validation-800.webp 800w,/assets/img/post_5_unet_validation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_unet_validation.png" class="img-fluid rounded" width="100%" height="auto" alt="Visual comparison between manually labeled validation data and the model‚Äôs predictions for artisanal/small-scale gold mining (ASGM) areas, highlighting the model's ability to differentiate between ASGM and other mining types like formal coal mining." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Visual comparison between the validation data labels and the model prediction. </div> <p>After the model get a good accuracy in the testing stage, the model are deployed for a time series analysis in the subset of the study area:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_timeseries_result-480.webp 480w,/assets/img/post_5_timeseries_result-800.webp 800w,/assets/img/post_5_timeseries_result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_timeseries_result.gif" class="img-fluid rounded" width="100%" height="auto" alt="Time-series classification animation of artisanal/small-scale gold mining (ASGM) areas, roads, and huts, as delineated by the deep learning model, visualizing the development of mining activities over time." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Timeseries classification result of ASGM areas, roads, and huts. </div> <p>Since this project also aimed to test early detection approach of mining activity, we included labels for hauling roads and mining huts, which often appear before intense mining activity begins. The model also able to deliniate huts and hauling roads as long as it is visible in the imagery by human eyes.</p> <h2 id="environmental-impact">Environmental Impact</h2> <p>As mentioned earlier, satellite imagery can also provide insights into various environmental impacts. Below, we show an empirical model of turbidity levels in rivers downstream of the previously modeled alluvial gold mining sites from Sentinel 2 imagery. By comparing the graph of mining activity with turbidity levels, we can observe a temporary pause in mining activity from late 2019 to 2021, likely due to the COVID-19 pandemic. During the same period, we also observed a decrease in river turbidity, suggesting a direct link between mining activity and river pollution.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_turbidity-480.webp 480w,/assets/img/post_5_turbidity-800.webp 800w,/assets/img/post_5_turbidity-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_turbidity.gif" class="img-fluid rounded" width="100%" height="auto" alt="Turbidity levels animation in rivers downstream of alluvial gold mining sites, observed through Sentinel-2 imagery, showing a decrease in turbidity levels following a pause in mining activity during the COVID-19 pandemic." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Turbidity level downstream of the mining area. </div> <h2 id="further-analysis-with-additional-geospatial-data-layers">Further Analysis with Additional Geospatial Data Layers</h2> <p>For legal and compliance analysis, a reasonable first step is to overlay the generated mining area outlines with other geospatial data layers linked to regulations governing mining activities in a specific country. For example, in Indonesia, we can use mining concession boundaries to determine whether mining activities are conducted within valid permit areas. We can also add data layers related to environmentally restricted areas, such as conservation zones, protected forests, and riparian zones, where mining is prohibited. This is a first-order approach to analyzing compliance levels. As previously stated, the complete regulatory framework for mining is complex, and further detailed analysis is needed for conclusive statements. However, this level of analysis is often sufficient to assess and estimate compliance at a national level.</p> <p>Below is an example of the national level overlay results in Indonesia using data from 2019. Note that the data used in this example are not official and may be incorrect, false, or inaccurate. Nevertheless, the purpose of this post is to demonstrate how compliance levels of mining activities can be assessed using remote sensing and basic geospatial analysis.The results of this data overlay include the percentage of mining areas inside and outside concession boundaries, as well as the percentage of mining areas within environmentally restricted zones.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_donut_chart-480.webp 480w,/assets/img/post_5_donut_chart-800.webp 800w,/assets/img/post_5_donut_chart-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_donut_chart.png" class="img-fluid rounded" width="100%" height="auto" alt="Donut chart displaying the percentage of mining areas located inside and outside of mining concession boundaries and environmentally restricted areas in Indonesia, based on data from 2019." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. Donut chart of the mining are classified by its location relative to concession area and environmentally restricted area. </div> <h2 id="complience-analysis">Complience Analysis</h2> <p>To better understand the overlay results, we need to consider relevant regulations. In this study, we incorporated data layers related to mining permits and land-use-based environmental restrictions. These layers help analyze the compliance of mining activities with legal and environmental regulations. Below are the key regulatory frameworks considered:</p> <ol> <li>According to <em>UU No. 4 Tahun 2009</em>, mining activities outside of their designated exploitation concession boundaries are strictly prohibited.</li> <li>According to <em>UU No. 41 Tahun 1999</em>, any land-use activity, including mining, is prohibited within conservation areas.</li> <li>According to <em>UU No. 41 Tahun 1999</em>, surface mining in protected forests is prohibited. Underground mining activity. However, underground mining may be allowed under specific conditions.</li> <li>According to <em>UU No. 41 Tahun 1999</em>, surface mining in production forests is prohibited. Nevertheless, special licenses may be granted to permit mining activities under additional requirements.</li> <li>According to <em>PP No. 38 Tahun 2011</em>, any land-use activity, including mining, is prohibited within riparian zones. However, special licenses may be issued to allow the redirection of river flows and riparian zones under certain conditions.</li> </ol> <p>based on these we will classify mining activity activity into three different complience category:</p> <ol> <li><strong>Category 1: High Compliance</strong> This is the highest compliance class. Mining areas in this category are located: <ul> <li>Within exploitation concession boundaries, and</li> <li>Outside any land-use-based environmental restriction areas.</li> </ul> </li> <li><strong>Category 2: Conditional Compliance</strong> Mining areas in this category are: <ul> <li>Within exploitation concession boundaries, but</li> <li>Located within production forests or riparian zones.</li> <li>If these mining activities have obtained the necessary additional permits to operate in production forests and/or redirect rivers, their compliance status aligns with Category 1.</li> </ul> </li> <li><strong>Category 3: Low Compliance</strong> This is the least compliance class in this analysis. The mining area are This is the least compliant category. Mining areas in this class are either: <ul> <li>Located within exploitation concession boundaries but also in protected forests or conservation areas, where mining is strictly prohibited, or</li> <li>Situated outside any concession boundaries, regardless of other restrictions.</li> </ul> </li> </ol> <p>The chart below visualize the categorization of the mining areas:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_classification_diagram-480.webp 480w,/assets/img/post_5_classification_diagram-800.webp 800w,/assets/img/post_5_classification_diagram-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_classification_diagram.gif" class="img-fluid rounded" width="100%" height="auto" alt="Diagram illustrating the classification of mining areas into three compliance categories: High Compliance, Conditional Compliance, and Low Compliance, based on the overlay of mining areas with concession boundaries and environmental restrictions." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. Complience category based on overlay results </div> <p>The result can also be visualized on province basis using pie chart map as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_category_pie_chart-480.webp 480w,/assets/img/post_5_category_pie_chart-800.webp 800w,/assets/img/post_5_category_pie_chart-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_category_pie_chart.png" class="img-fluid rounded" width="100%" height="auto" alt="Pie chart showing the percentage of mining areas within each compliance category across different provinces, providing a visual representation of compliance levels." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. Pie chart showing the percentage area of each category for every province. </div> <h2 id="further-analysis">Further analysis</h2> <p>I also think it would be interesting to combine this dataset with detailed census data. For example, I attempted to combine it with the WorldPop gridded population dataset to estimate the number of people living within a 10 km buffer of mining areas. This helps identify the potential population directly impacted by mining activities. Further analysis of health, education, and income data for this population could be valuable for future research.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_5_pop-480.webp 480w,/assets/img/post_5_pop-800.webp 800w,/assets/img/post_5_pop-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_5_pop.png" class="img-fluid rounded" width="100%" height="auto" alt="Map showing the total population living within a 10 km buffer zone around mining areas, using the WorldPop gridded population dataset to assess potential impacts on local communities. " onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8. Total population living within 10 km buffer of mining areas </div> <h2 id="conclussion">Conclussion</h2> <p>Satellite imagery provides extensive spatial and temporal coverage, making it a valuable tool for large-scale monitoring of terrestrial activities, including illegal mining. Using satellite data alone, we can extract information such as mining boundaries over time, mining methods, commodities, and environmental impacts. In our small-scale test, we demonstrated how deep learning can automate the mining boundary delineation process with good accuracy. To assess the compliance status of mining activities, additional geospatial data layers are required. In this example, we used unofficial mining concession boundaries and environmentally restricted area boundaries as an example. This simple yet effective approach enables high-scale monitoring to identify trends in mining activities and their compliance with regulations.</p>]]></content><author><name></name></author><category term="remote_sensing,"/><category term="monitoring,"/><category term="mining,"/><category term="security,"/><category term="data_science,"/><category term="machine_learning,"/><category term="deep_learning,"/><category term="multispectral,"/><category term="radar,"/><category term="GIS,"/><category term="environment"/><summary type="html"><![CDATA[Using satellite imagery to monitor illegal mining]]></summary></entry><entry><title type="html">Exploratory Data Analysis (EDA) for Hyperspectral Imagery</title><link href="https://nasirlukman.github.io/blog/2024/EDA/" rel="alternate" type="text/html" title="Exploratory Data Analysis (EDA) for Hyperspectral Imagery"/><published>2024-12-09T23:36:10+00:00</published><updated>2024-12-09T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2024/EDA</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2024/EDA/"><![CDATA[<p>One of the main characteristics of hyperspectral imagery is its high-dimensional data (i.e., a high number of spectral bands). This type of data, while providing a higher level of detail in the spectral characteristics of the material we observe, also raises challenges in handling the data effectively. Higher dimensional data means it becomes more challenging to extract meaningful information for analysis. These challenges are often referred to as <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">the curse of dimensionality</a>.</p> <p>In this post, I will show my approach in conducting exploratory data analysis (EDA) on a hyperspectral imagery with the goal of deriving meaningful geological information from the data. In this post, we will assume no geological prior knowledge of the region (although nowadays that is hardly the case, especially for Earth‚Äôs surface). We will rely solely on the hyperspectral data itself and some basic spectrometry and geology knowledge.</p> <p>We will use an airborne hyperspectral image covering the shortwave infrared range (SWIR) from ~2000 nm to ~2400 nm over an area of about 10 km¬≤ in an arid region. All of the analysis performed in this post is done using standard scientific libraries in the Python environment and an open-source software called <a href="http://hyppy.is-great.org/?i=1">Hyppy</a> developed at the University of Twente.</p> <h2 id="1-basic-visualization">1. Basic visualization</h2> <p>The obvious first step in EDA of hyperspectral imagery is to visualize the image. We can choose to visualize the image in grayscale for selected wavelength bands of interest and create false color composites to observe the main structural characteristics of the region.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_composite-480.webp 480w,/assets/img/post_4_composite-800.webp 800w,/assets/img/post_4_composite-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_composite.png" class="img-fluid rounded" width="100%" height="auto" alt="False color composite image of an arid terrain generated using three SWIR bands (Red: 2088 nm, Green: 2199 nm, Blue: 2328 nm). The image highlights geological features such as lineaments and varying lithologies. The composite enhances mineralogical and structural differences, useful for initial remote sensing interpretation." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. False color composite image (R: 2088 nm; G: 2199 nm; B: 2328 nm) </div> <p>My initial interpretation of the region is that it is most likely mainly composed of sedimentary bedding influenced by a fault with apparent left-lateral movement, based on the drag fold formed along the fault lines.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_basic_interpretation-480.webp 480w,/assets/img/post_4_basic_interpretation-800.webp 800w,/assets/img/post_4_basic_interpretation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_basic_interpretation.png" class="img-fluid rounded" width="100%" height="auto" alt="Image of annotated version of the false color composite showing interpreted geological features. Sedimentary bedding patterns are outlined, and a major left-lateral fault is identified with drag folds on both sides, suggesting tectonic deformation. Overlaid labels or arrows highlight bedding orientations and fault trace." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Basic geological feature initial interpretation from the image </div> <p>When it comes to mineralogical/geological interpretations, I find it helpful to begin with a very broad interpretation. Knowing that the region is likely predominantly composed of sedimentary rocks already provides significant constraints for further interpretation, which may be very useful in the next phase. We may revisit this interpretation later if further analysis or data does not support it.</p> <h2 id="2-getting-to-know-the-data">2. Getting to know the data</h2> <p>It is very important to familiarize ourselves with the data we are working with. At this stage, I typically inspect the spectra of some pixels that stand out in the visualization and get a sense of the variation in spectral responses present in the image. For this, I often use specialized software such as <a href="https://www.nv5geospatialsoftware.com/Products/ENVI">ENVI</a>, <a href="https://plugins.qgis.org/plugins/temporalprofiletool/">QGIS with Spectral Profile Plugin</a>, or <a href="http://hyppy.is-great.org/?i=1">Hyppy</a>. The later two is an open source program which are available for free.</p> <p>At this stage, we can also plot a histogram of the mean reflectance values of the image to see how many visible clusters are present across the spectral bands. Note that the reflectance value are in the sacale of 10,000.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_mean_histogram-480.webp 480w,/assets/img/post_4_mean_histogram-800.webp 800w,/assets/img/post_4_mean_histogram-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_mean_histogram.png" class="img-fluid rounded" width="100%" height="auto" alt="Image of histogram displaying the mean reflectance value of each pixel in the hyperspectral image. The horizontal axis shows reflectance (in scale of 10,000), and the vertical axis shows frequency of occurrence. Two distinct peaks indicate two main pixel groups, corresponding to low- and high-albedo surface materials." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Histogram of mean albedo of each pixel in the image showing two main albedo class. </div> <p>From the histogram, we can decide on a threshold to classify our pixels based on their average albedo. This is generally equivalent to classifying the image into bright-colored minerals and dark-colored minerals (albeit in the shortwave infrared range, not in the visible range). Typically, low-albedo minerals in SWIR also appear dark in visible light. Our first classification results are shown below which shows the most basic classification of the image we have: reflective vs absortive minerals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_albedo_class-480.webp 480w,/assets/img/post_4_albedo_class-800.webp 800w,/assets/img/post_4_albedo_class-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_albedo_class.png" class="img-fluid rounded" width="100%" height="auto" alt="This image shows binary classification map derived from average reflectance values in the SWIR range. Pixels are classified into two groups: high-albedo (bright minerals) and low-albedo (dark minerals), each shown in contrasting colors across the terrain. This reflects basic spectral segmentation for further analysis." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Map showing the distribution of two albedo class. </div> <h2 id="3-pca-and-k-means-clustering">3. PCA and K-Means Clustering</h2> <p>Since the number of pixels and spectral bands is often too high to handle for meaningful interpretation, dimensional reduction and data clustering methods can be useful for grouping data into clusters of spectra based on their spectral similarity which can make further analysis to be easier. While we can directly apply K-Means clustering to the data, this is not usually recommended. Each dimension of our data carries varying levels of information and noise. For high-dimensional data, only a few dimensions typically contain significant information, while others only add noise to the algorithm.</p> <p>PCA is a dimensionality reduction method that organizes data based on its variability in the data space, corresponding to its information content. It identifies the directions (principal components) where the data varies the most and projects the data onto these axes in descending order of variance. The downside is that the data is transformed into an orthogonal space where the units lose their original physical meaning. In this example, we visualize the first 10 principal components and observe that by the 10th component, there is minimal discriminatory power. Therefore, we reduce the dimensionality from 55 to 10 principal components, preserving the most significant information, and use this result as the input for K-Means clustering.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_PCA-480.webp 480w,/assets/img/post_4_PCA-800.webp 800w,/assets/img/post_4_PCA-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_PCA.png" class="img-fluid rounded" width="100%" height="auto" alt="Image of the grid layout showing the first 10 principal component images extracted from the hyperspectral dataset. Each panel represents a PC image where pixel intensity corresponds to the spectral variance captured in that component. The first few PCs show significant structure and contrast, while later PCs appear increasingly noisy and less informative." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. First 10 PCA images. </div> <p>The K-Means algorithm clusters data points based on their similarities. It assigns each point to one of K clusters based on its proximity to the cluster centroids, iterating to minimize variance within clusters. Similarities are computed using Euclidean distance, which I explained in a <a href="https://nasirlukman.github.io/blog/2024/distance/">previous post</a>.</p> <p>K-Means requires the user to define the number of clusters (\(K\) ). The elbow method can help determine this, by plotting the within-cluster sum of squares (WCSS) for various K values. The optimal number of clusters is where the WCSS curve starts to saturate, resembling an elbow. For this dataset, the optimal number of clusters is 3, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_WCSS-480.webp 480w,/assets/img/post_4_WCSS-800.webp 800w,/assets/img/post_4_WCSS-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_WCSS.png" class="img-fluid rounded" width="100%" height="auto" alt="Image of the line plot showing the elbow method applied to determine the optimal number of clusters for K-Means. The x-axis shows the number of clusters (K) from 1 to 11, and the y-axis shows the within-cluster sum of squares (WCSS). A noticeable bend at K=3 indicates the optimal number of clusters for this dataset." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. WCSS result for cluster values ranging from 1 to 11. </div> <p>Using \(K=3\) , we perform clustering, with the results shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_cluster_map-480.webp 480w,/assets/img/post_4_cluster_map-800.webp 800w,/assets/img/post_4_cluster_map-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_cluster_map.png" class="img-fluid rounded" width="100%" height="auto" alt="Map displaying K-Means clustering results on PCA-reduced hyperspectral data. Each pixel is colored according to its assigned spectral cluster (total of 3). The resulting clusters reflect differences in mineralogical composition or surface properties inferred from spectral similarities." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. K-Mean clustering result from the first 10 principle component of our hyperspectral images. </div> <p>We then can transform the pixels back from PCA space to the original data space to examine the average mean spectra of each cluster:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_cluster_spectra-480.webp 480w,/assets/img/post_4_cluster_spectra-800.webp 800w,/assets/img/post_4_cluster_spectra-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_cluster_spectra.png" class="img-fluid rounded" width="100%" height="auto" alt="Line graph showing average reflectance spectra for each of the three K-Means clusters. The x-axis represents wavelength in nanometers (2000‚Äì2400 nm), and the y-axis shows normalized reflectance values. The curves reveal distinct spectral features among the clusters, helping to distinguish different material types present in the area." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8. Average spectra of each cluster. </div> <p>Compared to our initial simple classification by mean albedo, this approach offers more nuance. For instance, the high-albedo reflectance now separates into two distinct spectral shapes, while the low-albedo minerals remain a single cluster. To further refine the low-albedo cluster, we could mask the high-albedo pixels and repeat the process. However, for this example, I am satisfied with these results. At this stage, I observe that the low-albedo cluster has largely featureless spectra (likely consisting of minerals without distinct features in the SWIR range), making further attempt for meaningful subdivision challenging.</p> <h2 id="4-endmember-extraction-and-linear-spectreal-unmixing">4. Endmember Extraction and Linear Spectreal Unmixing</h2> <p>Since our average spectra likely represent complex mixtures, it is useful to extract their possible pure spectral shapes. At this stage, we have decided to categorize our image into three distinct classes. We will use the <a href="">N-FINDR</a> algorithm with \(n=3\) to extract the three purest endmembers from our image. The data distribution in 2D PCA space and the extracted endmember pixels are shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_nfindr_plot-480.webp 480w,/assets/img/post_4_nfindr_plot-800.webp 800w,/assets/img/post_4_nfindr_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_nfindr_plot.png" class="img-fluid rounded" width="100%" height="auto" alt="2D PCA scatter plot showing a triangular distribution of hyperspectral data with three red stars at the vertices, indicating the locations of the purest endmember pixels." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 9. Data shape in reduced 2 PCA dimension showing clear linear relationship between three endmembers. Red star is the vertex of the triangle which corespond to the purest pixel on the image. </div> <p>This is an excellent 2D PCA representation. The 2D simplex (triangle) is almost perfectly formed, indicating that our entire dataset can be described as a linear combination of three endmember spectra. These endmembers therefore must be the pixel that correspond to the vertices of the triangle. After transforming the extracted endmembers back into the original data space, we observe their pure spectral signatures:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_endmember_spectra-480.webp 480w,/assets/img/post_4_endmember_spectra-800.webp 800w,/assets/img/post_4_endmember_spectra-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_endmember_spectra.png" class="img-fluid rounded" width="100%" height="auto" alt="Line graph showing the spectral signatures of three extracted endmembers across the SWIR wavelength range, each representing distinct mineral groups (mica, quartz, and carbonates)." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 10. Spectra of the purest pixels (endmembers). </div> <p>This is a good point to start naming our classes properly. You could match the spectra to a spectral library using a feature-matching algorithm to identify the mineral spectra most similar to the extracted endmembers. For simplicity, I use broad categorizations. I define the first spectrum as the <strong>Low-Albedo Mineral Group,</strong> likely dominated by plagioclase or mafic minerals, interpreted as part of a volcanic rock complex. The second spectrum corresponds to the <strong>Hydroxyl Mineral Group</strong>, based on its \(AlOH\) feature around 2200 nm, likely related to the occurance of white micas or clays, interpreted as part of siliciclastic rocks such as arkose. The third spectrum represents the <strong>Carbonate Mineral Group</strong>, with a \(CO_3\) feature near 2300 nm, likely from calcareous sedimentary rocks like limestone or calcareous sandstone/siltstone.</p> <p>With our pure endmembers identified, we can perform linear spectral unmixing to estimate their relative abundances. At this stage of analysis, I prefer to not constrain the unmixing result to be strictly sum-to-one since there are still many unknowns. These abundance results are interpreted as <em>unconstrained abundances</em>, distinct from <em>absolute abundances</em>, which have more physical meaning. Below are the unconstrained abundance maps for each endmember:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_unmixing_result-480.webp 480w,/assets/img/post_4_unmixing_result-800.webp 800w,/assets/img/post_4_unmixing_result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_unmixing_result.png" class="img-fluid rounded" width="100%" height="auto" alt="Three grayscale maps showing the unconstrained abundance distributions of each endmember across the study area, one map per endmember." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 11. Unconstrained abundance of the three endmembers. </div> <p>Since we are dealing with only three mineral groups, we can assign each abundance to an RGB channel to produce a ternary representation of the unconstrained abundances:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_unmixing_result_ternary_with_legend-480.webp 480w,/assets/img/post_4_unmixing_result_ternary_with_legend-800.webp 800w,/assets/img/post_4_unmixing_result_ternary_with_legend-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_unmixing_result_ternary_with_legend.png" class="img-fluid rounded" width="100%" height="auto" alt="Ternary abundance map combining the three endmembers into RGB channels, with a color legend showing how color blends represent relative mineral mixtures." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 12. Ternary map of the unconstrained abundance of the three endmembers. </div> <p>It is important to note that this ternary representation is purely for visualization and does not strictly correspond to the absolute abundance of each endmember.</p> <h2 id="5-minimum-wavelength-mapping">5. Minimum Wavelength Mapping</h2> <p>Many distinct hyroxyl and carboante minerals can be distinguished based on the subtle difference in the wavelength position of their deepest absorption feature. Using a method called minimum wavelength mapping, we can detect these subtle shifts and potentially refine our classification to include specific members of the carbonate and hydroxyl mineral groups. The first step is to detect the wavelength of the deepest absorption feature across the entire spectral range.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_minwav-480.webp 480w,/assets/img/post_4_minwav-800.webp 800w,/assets/img/post_4_minwav-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_minwav.png" class="img-fluid rounded" width="100%" height="auto" alt="Histogram displaying the distribution of minimum absorption wavelengths across the full spectral range, highlighting multiple peaks for AlOH and CO‚ÇÉ features." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 13. Minimum wavelength position histogram for the whole spectral range. </div> <p>We observe at least three distinct distributions of \(AlOH\) absorption features and five distributions of \(CO_3\) absorption features. For the \(AlOH\) group, the lowest feature observed at 2195 nm likely corresponds to lithium-rich white mica, such as lepidolite. The other two \(AlOH\) features, around 2210 nm and 2220 nm, likely correspond to \(Al\)-rich white mica, such as muscovite, with minor geochemical differences attributed to variations of geochemistry and temperature of the mineral formation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_AlOH_minwav_legend-480.webp 480w,/assets/img/post_4_AlOH_minwav_legend-800.webp 800w,/assets/img/post_4_AlOH_minwav_legend-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_AlOH_minwav_legend.png" class="img-fluid rounded" width="100%" height="auto" alt="Spatial map of minimum wavelength positions for AlOH absorption features, color-coded to distinguish at least three hydroxyl-bearing mineral subgroups." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 14. Minimum wavelength map of hydroxile features. </div> <p>For the \(CO_3\) group, there is significant variation in the deepest feature wavelengths. Although identifying specific minerals at this stage is challenging, the observed ranges suggest these features are unlikely to correspond to calcite, the most common carbonate mineral. For instance, features around 2335 nm are commonly associated with siderite, 2320 nm with dolomite, and 2305 nm with magnesite.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_4_CO3_minwav_legend-480.webp 480w,/assets/img/post_4_CO3_minwav_legend-800.webp 800w,/assets/img/post_4_CO3_minwav_legend-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_4_CO3_minwav_legend.png" class="img-fluid rounded" width="100%" height="auto" alt="Spatial map of minimum wavelength positions for CO‚ÇÉ absorption features, with color coding representing possible variations in carbonate mineral types." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 15. Minimum wavelength map of carbonate features. </div> <p>It is essential to note that pinpointing specific minerals requires further analysis. While we cannot yet confirm the mineral species, the variations in these wavelength ranges are likely related to mineralogical differences. Therefore, the spatial distribution of these distinct groups remains valid, even when we are still uncertain with the definitive mineral identifications.</p> <h2 id="6-further-comments">6. Further comments</h2> <p>In this post, I demonstrated some typical EDA steps for hyperspectral analysis for geological investigations in an unfamiliar region. These EDA steps serve to familiarize us with the data and provide a solid foundation for further investigation.</p> <p>Here we see that SWIR hyperspectral data alone can reveal a wealth of information, even without incorporating additional datasets, prior knowledge of the region, or advanced analysis techniques. However, complementary datasets, such as VNIR and LWIR spectral data, could significantly enhance geological and mineralogical interpretation of the region. Additionally, methods beyond infrared spectrometry, such as gamma-ray spectrometry, offer valuable tools for geological mapping.</p> <p>In practice, prior knowledge of the study area‚Äîsuch as geological maps or models‚Äîis often available and can effectively guide the analysis. More importantly, incorporating fieldwork data, even from sparse sampling points, is crucial for validating and refining the results.</p>]]></content><author><name></name></author><category term="spectral_geology,"/><category term="mineral,"/><category term="data_science,"/><category term="geology,"/><category term="hyperspectral,"/><category term="remote_sensing"/><summary type="html"><![CDATA[My typical approach on EDA for a new hyperspectral projects for geological investigations. Includes method such as PCA, K-Means, N-FINDR, Linear Spectral Unmixing, and Minimum Wavelength Mapping.]]></summary></entry></feed>