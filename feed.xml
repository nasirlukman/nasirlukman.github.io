<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nasirlukman.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nasirlukman.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-18T10:01:56+00:00</updated><id>https://nasirlukman.github.io/feed.xml</id><title type="html">Earth.etc</title><subtitle></subtitle><entry><title type="html">Different Distance Metrics Example in Spectral Unmixing</title><link href="https://nasirlukman.github.io/blog/2024/distance/" rel="alternate" type="text/html" title="Different Distance Metrics Example in Spectral Unmixing"/><published>2024-11-15T23:36:10+00:00</published><updated>2024-11-15T23:36:10+00:00</updated><id>https://nasirlukman.github.io/blog/2024/distance</id><content type="html" xml:base="https://nasirlukman.github.io/blog/2024/distance/"><![CDATA[<p>In data space, distance typically measures similarity between data points. For instance, if points <em>a</em> and <em>b</em> are “close” to each other (i.e., have a small distance), they are considered more similar. When distance equals zero, data points <em>a</em> and <em>b</em> are exactly the same. This concept is essential in machine learning and data analysis as it quantifies similarity between datasets.</p> <p>In two dimensions, it’s easy to visualize data points and assess their distances, such as in this example:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_image_1_three_point_distance-480.webp 480w,/assets/img/post_1_image_1_three_point_distance-800.webp 800w,/assets/img/post_1_image_1_three_point_distance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_image_1_three_point_distance.jpg" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1. Three points in two dimensional space, illustrating the concept of distance. </div> <p>Visually, most of us would agree that points <em>a</em> and <em>b</em> appear closer in distance to each other compared to <em>a</em> and <em>c</em>. But is this really the case? To confirm, we need to define what we mean by “distance” and calculate the distance between each point.</p> <p>In this post, we will examine three different distance metrics and compare their performance in a hypothetical spectral unmixing problem: Euclidean distance, Manhattan distance, and angular distance. I think it is easier to first take a look the visualization of how each metrics measure distance:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_image_2_three_type_of_distance-480.webp 480w,/assets/img/post_1_image_2_three_type_of_distance-800.webp 800w,/assets/img/post_1_image_2_three_type_of_distance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_image_2_three_type_of_distance.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2. Comparison of how Euclidean, Manhattan, and Angular distances measure relationships between points. </div> <p>I think this ilustration cleary shows the distinction between how the three metrics measure distance, to formalize this, let’s go to each metrics and see its mathematical foromulation.</p> <h3 id="euclidean-distance">Euclidean Distance</h3> <p>The most common and intuitive distance is Euclidean distance, also often referred to as \(L_2\) distance. Between two points <em>a</em> and <em>b</em> in data space, Euclidean distance is simply the straight-line distance between the points, which can be expressed mathematically as:</p> \[L_2 = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\] <h3 id="manhattan-distance">Manhattan Distance</h3> <p>Another way to measure distance from \(a\) to \(b\) is to follow a path parallel to the axis. For example, in the image below, we first move parallel to the y-axis for 1 units, followed by a parallel path along the x-axis for 1 units, and then sum the total path length. This type of “gridded” path is called Manhattan distance (after the gridded road pattern of Manhattan) or \(L_1\) distance. Mathematically, it can be expressed as:</p> \[L_1 = \sum_{i=1}^n |a_i - b_i|\] <h3 id="angular-distance">Angular Distance</h3> <p>For angular distance \((\theta)\), we treat points \(a\) and \(b\) as two vectors originating from the origin \((0,0)\). The angular distance is simply the angle between the two vectors, expressed as:</p> \[\theta = \arccos \left( \frac{a \cdot b}{\|a\| \|b\|} \right)\] <p>In machine learning, angular distance is more commonly referred to as cosine similarity (measured as the cosine of the angle). In spectral analysis, this is often called Spectral Angle Mapping (SAM) and is widely used for spectral matching. Mathematically, angular distance is only valid as a metric if we normalize our data to unit vectors. For unit vectors, this measurement gives the same result as Euclidean distance after normalizing the data.</p> <p>From these definitions of the three distance metrics, lets return to our initial visualization (Image 1): Is point <em>a</em> closer to point <em>b</em> than to point <em>c</em> ? Again, the answer depends on the distance metric. Using Euclidean and Manhattan distances, this statement holds true. However, with angular distance, it is actually false, while <em>a</em> and <em>b</em> have a non-zero angular distance, <em>a</em> and <em>c</em> actually have an angular distance of \(0\)! So <em>c</em> is closer to <em>a</em> compared to <em>b</em>.</p> <h2 id="example-case-linear-spectral-unmixing">Example Case: Linear Spectral Unmixing</h2> <p>To illustrate cases in which each distance metric is more appropriate, let’s use linear spectral unmixing as an example. Spectral unmixing is a critical technique in remote sensing used to decompose mixed pixel values into their constituent spectral signatures, known as endmembers, and their respective abundances. This process is essential for applications requiring sub-pixel level analysis, such as land cover classification, resource exploration, and environmental monitoring.</p> <p>An important aspect of spectral unmixing is the choice of distance metrics, which measure the similarity or dissimilarity between observed pixel spectra and modeled spectra. The selection of an appropriate metric significantly impacts the accuracy and performance of unmixing algorithms. Understanding these metrics helps optimize the unmixing process, especially in scenarios with high spectral variability or noise.</p> <p>Assuming a linear mixture model, a mixed spectrum of several different endmembers is defined as:</p> \[y = E a + n\] <p>where:</p> <ul> <li>\(y\) is the observed spectrum,</li> <li>\(E\) is the endmembers matrix,</li> <li>\(a\) is the abundance vector,</li> <li>\(n\) represents noise.</li> </ul> <p>In this equation, \(y\) is known from observation, and for simplicity, we assume that a matrix of possible endmembers \(E\) is also known, and noise \(n\) is negligible.</p> <p>The goal is to solve for \(a\) by minimizing the difference between the modeled spectrum \(E a\) and the observed spectrum \(y\), under the conditions that all values of \(a\) are non-negative and summed-to-one. This may achieved by solving this optimization problem:</p> \[\min_{\mathbf{a}} \; D(\mathbf{y} - \mathbf{E} \mathbf{a})\] <p>subject to:</p> \[a_i \geq 0 \quad \text{and} \quad \sum_{i=1}^{n} a_i = 1\] <p>This is where distance metrics \(D(•)\) come into play. The difference we are trying to minimize can be calculated using different distance metrics. Different distance metrics can lead to different results.</p> <p>Now, let’s run some tests under various simplified scenarios to see how each distance metric performs.</p> <h3 id="test-01-mixture-with-gaussian-noise">Test 01: Mixture with Gaussian Noise</h3> <p>For the first test, suppose we have four mineral endmembers (example taken from USGS Spectral Library), as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_image_3_four_endmembers-480.webp 480w,/assets/img/post_1_image_3_four_endmembers-800.webp 800w,/assets/img/post_1_image_3_four_endmembers-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_image_3_four_endmembers.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3. Endmember spectra for Calcite, Kaolinite, Muscovite, and Quartz </div> <p>We’ll generate 1000 synthetic mixtures with the following conditions:</p> <ol> <li>The mixtures follow the linear mixture model equation.</li> <li>Noise is normally distributed with a mean of \(0\) and a standard deviation chosen randomly between \(10^{-5}\) and \(10^{-3}\).</li> </ol> <p>The accuracy of the spectral unmixing using the three distance metrics are shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_first_test-480.webp 480w,/assets/img/post_1_first_test-800.webp 800w,/assets/img/post_1_first_test-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_first_test.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Hisotgram showing the accuracy (RMSE) of 1000 sepctral unmixing using three different distance metrics </div> <p>In this case, <strong>Euclidean distance</strong> clearly outperforms the other two metrics, as it is well-suited for data with normally distributed noise. Under these conditions, minimizing Euclidean distance is equivalent to maximum likelihood estimation.</p> <h3 id="test-02-mixture-with-random-spikes">Test 02: Mixture with Random ‘Spikes’</h3> <p>For the second test, instead of adding normal noise, this time we add random “spikes,” where each wavelength band has a \(1%\) chance of being randomly increased or decreased by a value between \(0.05\) and \(0.2\). While this type of noise is not common in remote sensing or spectroscopy, it will help us illustrate the strengths of Manhattan distance. The accuracy of the spectral unmixing of these synthetic mixtures are shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_second_test-480.webp 480w,/assets/img/post_1_second_test-800.webp 800w,/assets/img/post_1_second_test-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_second_test.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4. Hisotgram showing the accuracy (RMSE) of 1000 sepctral unmixing using three different distance metrics </div> <p>Here, <strong>Manhattan distance</strong> outperforms the other metrics. Manhattan distance is known to be more robust to extreme outliers than Euclidean distance, which squares the error term, making it sensitive to outliers. In contrast, Manhattan distance only takes the absolute value of location differences, making it less affected by outliers.</p> <h3 id="test-03-mixture-with-brightness-difference">Test 03: Mixture with Brightness Difference</h3> <p>While random spikes in reflectance values may not be common in spectroscopy or remote sensing, brightness differences and inaccurate endmembers spectra are issues that often arise with real datasets. To illustrate this problem, let’s randomly add some constant to the mixture in hte range between 0 to 0.2. This experiment is a simple simulation of real world scenario where different terrains and illumination condition might cause te sensor to recieve the signal with different intensity if not corrected appropriately.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_third_test-480.webp 480w,/assets/img/post_1_third_test-800.webp 800w,/assets/img/post_1_third_test-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_third_test.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5. Hisotgram showing the accuracy (RMSE) of 1000 sepctral unmixing using three different distance metrics </div> <p>In this case, <strong>angular distance</strong> (or SAM) has the lowest error, as it essentially ignores magnitude (brightness) differences and focuses on the shape of the spectra. To illustrate further, take a look at the two spectra below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_kaolinite-480.webp 480w,/assets/img/post_1_kaolinite-800.webp 800w,/assets/img/post_1_kaolinite-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_kaolinite.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6. Two kaolinite spectra with different brightness </div> <p>The two spectra above have identical shape but different brightness values. These two spectra may be considered different by Manhattan and Euclidean distances, but angular distance would see them as identical. This feature is one reason why angular distance is widely used in remote sensing applications, where brightness differences due to terrains and illuminations effects are common.</p> <p>Also note that from these example trials, the error that we got is still very large. This was largerly due to the fact that a simple optimization approach that we take simply assume that the data and the model is perfect and ignores the noise/error term \(n\). In the next post we will try to adress this with a Bayesian inversion approach of spectral unmixing.</p> <h3 id="test-04-a-more-complicated-mixture">Test 04: A More Complicated Mixture</h3> <p>Now consider a more complicated scenarion. First we will consider larger number of spectral library. We will take 10 mineral from usgs spectra library, and randomly take betweeen 2 to 4 endmembers to generate a mixture. As the selected mineral is unknown, the whole 10 mineral endmembers will be considered during the unmixing.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_10_endmembers-480.webp 480w,/assets/img/post_1_10_endmembers-800.webp 800w,/assets/img/post_1_10_endmembers-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_10_endmembers.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7. 10 Endmembers Spectra </div> <p>We will also add an gaussian noise, uniform noise, gamma noise, poisson noise, and random constant additive noise as brightness modifier. The image below ilustrate each of the noises:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_noises-480.webp 480w,/assets/img/post_1_noises-800.webp 800w,/assets/img/post_1_noises-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_noises.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8. Each noises added to the mixture. </div> <p>The unmixing accuracy results are shown in histograms below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_1_forth_test-480.webp 480w,/assets/img/post_1_forth_test-800.webp 800w,/assets/img/post_1_forth_test-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_1_forth_test.png" class="img-fluid rounded" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 9. Hisotgram showing the accuracy (RMSE) of 1000 sepctral unmixing using three different distance metrics </div> <p><strong>Manhattan distance</strong> performed exceptionally well in this last case study, which probably can be attributed to its robustness to ouliers and more varying noise types. Despite this, its non-differentiable nature (since its an absolute value function) makes it less appealing for gradient-based optimization algorithms, especially for larger, more complex datasets. For this particular test, Manhattan distance takes 43 seconds for 1,000 data points, compared to 14 seconds for Euclidean distance and 11 seconds for angular distance.</p> <p>This study highlights the importance of carefully selecting the appropriate distance metric based on the nature of the data and the problem being addressed. While Euclidean distance is often the default choice, Manhattan distance or angular distance can outperform it in specific scenarios, as demonstrated in the experiments above.</p> <p>On a final note I think it is important to be explicit here that the unmixing method that is performed here conducted using scipy.optimize library and the fundamental assumption of the approach is that the data and the model is perfect. In real data scenario, that is not the case, so in general we should expect the result will be slighty worse. There are ways to improve the results, for example adding some regularization terms, especially if we try do it using quite large selection of endmembers spectra such as the last example. But realisticly we should understand that spectral unmixing involves in a lot of uncertainty. A more natural way to deal with this type of problems is by using a <em>Bayesian framework</em>, which might be the topic of the next post.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Comparison between Euclidean distance, Manhattan distance, and angular distance in spectral unmixing]]></summary></entry></feed>